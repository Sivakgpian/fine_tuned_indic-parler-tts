{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyVqUV0t0Jae",
        "outputId": "7afdfb8e-2c54-4ba6-e645-e48bd4385a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'parler-tts'...\n",
            "remote: Enumerating objects: 1100, done.\u001b[K\n",
            "remote: Counting objects: 100% (457/457), done.\u001b[K\n",
            "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
            "remote: Total 1100 (delta 344), reused 280 (delta 280), pack-reused 643 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1100/1100), 356.63 KiB | 1.37 MiB/s, done.\n",
            "Resolving deltas: 100% (702/702), done.\n",
            "/content/parler-tts\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/huggingface/parler-tts.git\n",
        "%cd parler-tts/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0div0p6wduBp",
        "outputId": "5f86701e-7281-463e-de34-853157952786"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npip install pathos\\npip install datasets\\n!pip install parler-tts\\npip install evaluate\\npip install jiwer '"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "pip install pathos\n",
        "pip install datasets\n",
        "!pip install parler-tts\n",
        "pip install evaluate\n",
        "pip install jiwer '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5SPpFrDdsE7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BQLqSBH0Jd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "holFOYwOXEBM",
        "outputId": "a6e8d0ac-8bcb-40d0-f240-0dd15d04ea5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pathos\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ppft>=1.7.6.9 (from pathos)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.9 (from pathos)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.5 (from pathos)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ppft, pox, dill, multiprocess, pathos\n",
            "Successfully installed dill-0.3.9 multiprocess-0.70.17 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9\n"
          ]
        }
      ],
      "source": [
        "pip install pathos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jkmq8PB3XPUD",
        "outputId": "09a7f9e4-1bba-4eaa-abb3-610f3fd36e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.9\n",
            "    Uninstalling dill-0.3.9:\n",
            "      Successfully uninstalled dill-0.3.9\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.17\n",
            "    Uninstalling multiprocess-0.70.17:\n",
            "      Successfully uninstalled multiprocess-0.70.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.8 which is incompatible.\n",
            "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.16 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4KdDJF69XKSU",
        "outputId": "4904905e-d233-4f1c-f361-8f663d61ef74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting parler-tts\n",
            "  Downloading parler_tts-0.2.3.tar.gz (80 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/80.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<=4.46.1,>=4.46.1 (from parler-tts)\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from parler-tts) (2.6.0+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from parler-tts) (0.2.0)\n",
            "Collecting descript-audio-codec-unofficial (from parler-tts)\n",
            "  Downloading descript_audio_codec_unofficial-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting descript-audiotools-unofficial (from parler-tts)\n",
            "  Downloading descript_audiotools_unofficial-0.7.4.tar.gz (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.7/100.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from parler-tts) (5.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (0.5.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers<=4.46.1,>=4.46.1->parler-tts)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.46.1,>=4.46.1->parler-tts) (4.67.1)\n",
            "Collecting argbind>=0.3.7 (from descript-audio-codec-unofficial->parler-tts)\n",
            "  Downloading argbind-0.3.9.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from descript-audio-codec-unofficial->parler-tts) (0.8.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from descript-audio-codec-unofficial->parler-tts) (2.6.0+cu124)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (0.13.1)\n",
            "Collecting pyloudnorm (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (6.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (1.14.1)\n",
            "Collecting julius (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpy (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (7.34.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (13.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (0.11.0)\n",
            "Collecting pystoi (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting torch_stoi (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading torch_stoi-0.2.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting flatten-dict (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting markdown2 (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting randomname (from descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading randomname-0.2.1.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting protobuf>=4.0.0 (from parler-tts)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from descript-audiotools-unofficial->parler-tts) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->parler-tts)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->parler-tts)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->parler-tts)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->parler-tts)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->parler-tts)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->parler-tts)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->parler-tts)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->parler-tts)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->parler-tts)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->parler-tts)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->parler-tts) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->parler-tts) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser in /usr/local/lib/python3.11/dist-packages (from argbind>=0.3.7->descript-audio-codec-unofficial->parler-tts) (0.16)\n",
            "Requirement already satisfied: six<2.0,>=1.12 in /usr/local/lib/python3.11/dist-packages (from flatten-dict->descript-audiotools-unofficial->parler-tts) (1.17.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->descript-audiotools-unofficial->parler-tts) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->parler-tts) (3.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (1.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->descript-audiotools-unofficial->parler-tts) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->descript-audiotools-unofficial->parler-tts) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->descript-audiotools-unofficial->parler-tts) (2.8.2)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from pyloudnorm->descript-audiotools-unofficial->parler-tts) (1.0.0)\n",
            "Collecting fire (from randomname->descript-audiotools-unofficial->parler-tts)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.46.1,>=4.46.1->parler-tts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.46.1,>=4.46.1->parler-tts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.46.1,>=4.46.1->parler-tts) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.46.1,>=4.46.1->parler-tts) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->descript-audiotools-unofficial->parler-tts) (3.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->descript-audiotools-unofficial->parler-tts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->descript-audiotools-unofficial->parler-tts) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->descript-audiotools-unofficial->parler-tts) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->descript-audiotools-unofficial->parler-tts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->descript-audiotools-unofficial->parler-tts) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->descript-audiotools-unofficial->parler-tts) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->descript-audiotools-unofficial->parler-tts) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->descript-audiotools-unofficial->parler-tts) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->descript-audiotools-unofficial->parler-tts) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->descript-audiotools-unofficial->parler-tts) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->descript-audiotools-unofficial->parler-tts) (4.3.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->descript-audiotools-unofficial->parler-tts) (0.2.13)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->descript-audiotools-unofficial->parler-tts) (3.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->randomname->descript-audiotools-unofficial->parler-tts) (2.5.0)\n",
            "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Downloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading torch_stoi-0.2.3-py3-none-any.whl (8.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: parler-tts, descript-audio-codec-unofficial, descript-audiotools-unofficial, argbind, julius, randomname, fire\n",
            "  Building wheel for parler-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parler-tts: filename=parler_tts-0.2.3-py3-none-any.whl size=81610 sha256=11e821dc5719a2a45fefc196bf192a0bc34e5ab3926c3aa6a946328b0730ffc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/94/4a/89d2bbf31af3caa0b79ef47a0823224b7e2ddae2febc10920b\n",
            "  Building wheel for descript-audio-codec-unofficial (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for descript-audio-codec-unofficial: filename=descript_audio_codec_unofficial-1.0.0-py3-none-any.whl size=27053 sha256=1b49c0bc748c6f4ee86c1be18714d6b83459fc773cd8e905c6a7a55c173c501a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/18/5f/17a643fe763770d2451bb1ee893c188fe5680288c28c238fcf\n",
            "  Building wheel for descript-audiotools-unofficial (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for descript-audiotools-unofficial: filename=descript_audiotools_unofficial-0.7.4-py2.py3-none-any.whl size=108046 sha256=d30415f5362eb291037ca3ba303c2e7b134cee0e50c815d9ac3e567fdfdb3955\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/ed/8f/fec8ded5f11f4b4a0bfd716b7516b7a09a9aacaf5d19acb3da\n",
            "  Building wheel for argbind (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for argbind: filename=argbind-0.3.9-py2.py3-none-any.whl size=11729 sha256=1e10c918c9ab78e07b1ea7ee42771d8f1677aea0f6cf61db453db06eaeeca4b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/3a/34/e858fa3cf5f8c33a040734efcc17e95cb5cfd99c256a7fcecf\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=c594a2a1efffed653e2902b6aac2efe1170233caf38d2ad01aeccf5b3f51017b\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n",
            "  Building wheel for randomname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for randomname: filename=randomname-0.2.1-py3-none-any.whl size=89195 sha256=fb3370e873c8e0ca51b7230cabcb614e923c6529d4b2f8ffc20ef23cab93544b\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/b3/ae/c137ed34d7c385b74ae440b4f008183264ebe466ea0341db09\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=a372756a3b5d736417201bb33c8d8fae30b405a557e5703c6f32635f19005fa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built parler-tts descript-audio-codec-unofficial descript-audiotools-unofficial argbind julius randomname fire\n",
            "Installing collected packages: protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown2, jedi, flatten-dict, fire, ffmpy, argbind, randomname, pystoi, pyloudnorm, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, julius, torch_stoi, descript-audiotools-unofficial, descript-audio-codec-unofficial, parler-tts\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.3\n",
            "    Uninstalling protobuf-5.29.3:\n",
            "      Successfully uninstalled protobuf-5.29.3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.49.0\n",
            "    Uninstalling transformers-4.49.0:\n",
            "      Successfully uninstalled transformers-4.49.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed argbind-0.3.9 descript-audio-codec-unofficial-1.0.0 descript-audiotools-unofficial-0.7.4 ffmpy-0.5.0 fire-0.7.0 flatten-dict-0.4.2 jedi-0.19.2 julius-0.2.7 markdown2-2.5.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 parler-tts-0.2.3 protobuf-4.25.6 pyloudnorm-0.1.1 pystoi-0.4.1 randomname-0.2.1 tokenizers-0.20.3 torch_stoi-0.2.3 transformers-4.46.1\n"
          ]
        }
      ],
      "source": [
        "!pip install parler-tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OmwcHaTCYSbk",
        "outputId": "c59fde92-6964-429d-aabb-645d38b31122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "759qWQkeS4Ph",
        "outputId": "319645a3-ed5c-4006-a752-557911c6c28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.12.2\n"
          ]
        }
      ],
      "source": [
        "pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L36OmqmfR_Bi",
        "outputId": "6294af75-95cc-46b3-896e-68af3ca820c3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/parler-tts'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArqCbPgO4O1U",
        "outputId": "2ab39e95-e666-4345-d986-fe3cb7d03dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/parler-tts\n"
          ]
        }
      ],
      "source": [
        "%cd /content/parler-tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucnGFn7pHkZi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhYSd_dg2f1q"
      },
      "outputs": [],
      "source": [
        "'''if \"bandwidth\" in batch:\n",
        "        del batch[\"bandwidth\"]'''\n",
        "# change in line436 in run_parler_tts_training.py\n",
        "\n",
        "# dimensionality error at line no 450\n",
        "'''\n",
        "def apply_audio_decoder(batch):\n",
        "    len_audio = batch.pop(\"len_audio\")\n",
        "    audio_decoder.to(batch[\"input_values\"].device).eval()\n",
        "\n",
        "    # Handle encoder-specific parameters\n",
        "    if \"bandwidth\" in batch:\n",
        "        del batch[\"bandwidth\"]\n",
        "    elif \"num_quantizers\" in encoder_signature:\n",
        "        batch[\"num_quantizers\"] = num_codebooks\n",
        "    elif \"num_codebooks\" in encoder_signature:\n",
        "        batch[\"num_codebooks\"] = num_codebooks\n",
        "    elif \"n_quantizers\" in encoder_signature:\n",
        "        batch[\"n_quantizers\"] = num_codebooks\n",
        "\n",
        "    with torch.no_grad():\n",
        "        labels = audio_decoder.encode(**batch)[\"audio_codes\"]\n",
        "\n",
        "    output = {}\n",
        "    output[\"len_audio\"] = len_audio\n",
        "\n",
        "    # Check the dimensions of the labels tensor before squeezing and transposing\n",
        "    if labels.dim() == 4:\n",
        "        # If the tensor has 4 dimensions, proceed with squeezing and transposing\n",
        "        output[\"labels\"] = labels.squeeze(0).transpose(1, 2)\n",
        "    elif labels.dim() == 3:\n",
        "        # If the tensor has 3 dimensions, directly transpose without squeezing\n",
        "        output[\"labels\"] = labels.transpose(1, 2)\n",
        "    else:\n",
        "        # Handle unexpected tensor dimensions here\n",
        "        raise ValueError(f\"Unexpected tensor shape for labels: {labels.shape}\")\n",
        "\n",
        "    # if `pad_to_max_length`, the maximum corresponding audio length of the current batch is max_duration*sampling_rate\n",
        "    max_length = len_audio.max() if padding != \"max_length\" else max_target_length\n",
        "    output[\"ratio\"] = torch.ones_like(len_audio) * labels.shape[-1] / max_length\n",
        "\n",
        "    return output\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68xZI-c1a4-k",
        "outputId": "bf8b9f4a-625e-4e1b-9a87-cfd51e732ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ]\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at distil-whisper/distil-large-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"is_multilingual\": true,\n",
            "  \"lang_to_id\": {\n",
            "    \"<|af|>\": 50327,\n",
            "    \"<|am|>\": 50334,\n",
            "    \"<|ar|>\": 50272,\n",
            "    \"<|as|>\": 50350,\n",
            "    \"<|az|>\": 50304,\n",
            "    \"<|ba|>\": 50355,\n",
            "    \"<|be|>\": 50330,\n",
            "    \"<|bg|>\": 50292,\n",
            "    \"<|bn|>\": 50302,\n",
            "    \"<|bo|>\": 50347,\n",
            "    \"<|br|>\": 50309,\n",
            "    \"<|bs|>\": 50315,\n",
            "    \"<|ca|>\": 50270,\n",
            "    \"<|cs|>\": 50283,\n",
            "    \"<|cy|>\": 50297,\n",
            "    \"<|da|>\": 50285,\n",
            "    \"<|de|>\": 50261,\n",
            "    \"<|el|>\": 50281,\n",
            "    \"<|en|>\": 50259,\n",
            "    \"<|es|>\": 50262,\n",
            "    \"<|et|>\": 50307,\n",
            "    \"<|eu|>\": 50310,\n",
            "    \"<|fa|>\": 50300,\n",
            "    \"<|fi|>\": 50277,\n",
            "    \"<|fo|>\": 50338,\n",
            "    \"<|fr|>\": 50265,\n",
            "    \"<|gl|>\": 50319,\n",
            "    \"<|gu|>\": 50333,\n",
            "    \"<|haw|>\": 50352,\n",
            "    \"<|ha|>\": 50354,\n",
            "    \"<|he|>\": 50279,\n",
            "    \"<|hi|>\": 50276,\n",
            "    \"<|hr|>\": 50291,\n",
            "    \"<|ht|>\": 50339,\n",
            "    \"<|hu|>\": 50286,\n",
            "    \"<|hy|>\": 50312,\n",
            "    \"<|id|>\": 50275,\n",
            "    \"<|is|>\": 50311,\n",
            "    \"<|it|>\": 50274,\n",
            "    \"<|ja|>\": 50266,\n",
            "    \"<|jw|>\": 50356,\n",
            "    \"<|ka|>\": 50329,\n",
            "    \"<|kk|>\": 50316,\n",
            "    \"<|km|>\": 50323,\n",
            "    \"<|kn|>\": 50306,\n",
            "    \"<|ko|>\": 50264,\n",
            "    \"<|la|>\": 50294,\n",
            "    \"<|lb|>\": 50345,\n",
            "    \"<|ln|>\": 50353,\n",
            "    \"<|lo|>\": 50336,\n",
            "    \"<|lt|>\": 50293,\n",
            "    \"<|lv|>\": 50301,\n",
            "    \"<|mg|>\": 50349,\n",
            "    \"<|mi|>\": 50295,\n",
            "    \"<|mk|>\": 50308,\n",
            "    \"<|ml|>\": 50296,\n",
            "    \"<|mn|>\": 50314,\n",
            "    \"<|mr|>\": 50320,\n",
            "    \"<|ms|>\": 50282,\n",
            "    \"<|mt|>\": 50343,\n",
            "    \"<|my|>\": 50346,\n",
            "    \"<|ne|>\": 50313,\n",
            "    \"<|nl|>\": 50271,\n",
            "    \"<|nn|>\": 50342,\n",
            "    \"<|no|>\": 50288,\n",
            "    \"<|oc|>\": 50328,\n",
            "    \"<|pa|>\": 50321,\n",
            "    \"<|pl|>\": 50269,\n",
            "    \"<|ps|>\": 50340,\n",
            "    \"<|pt|>\": 50267,\n",
            "    \"<|ro|>\": 50284,\n",
            "    \"<|ru|>\": 50263,\n",
            "    \"<|sa|>\": 50344,\n",
            "    \"<|sd|>\": 50332,\n",
            "    \"<|si|>\": 50322,\n",
            "    \"<|sk|>\": 50298,\n",
            "    \"<|sl|>\": 50305,\n",
            "    \"<|sn|>\": 50324,\n",
            "    \"<|so|>\": 50326,\n",
            "    \"<|sq|>\": 50317,\n",
            "    \"<|sr|>\": 50303,\n",
            "    \"<|su|>\": 50357,\n",
            "    \"<|sv|>\": 50273,\n",
            "    \"<|sw|>\": 50318,\n",
            "    \"<|ta|>\": 50287,\n",
            "    \"<|te|>\": 50299,\n",
            "    \"<|tg|>\": 50331,\n",
            "    \"<|th|>\": 50289,\n",
            "    \"<|tk|>\": 50341,\n",
            "    \"<|tl|>\": 50348,\n",
            "    \"<|tr|>\": 50268,\n",
            "    \"<|tt|>\": 50351,\n",
            "    \"<|uk|>\": 50280,\n",
            "    \"<|ur|>\": 50290,\n",
            "    \"<|uz|>\": 50337,\n",
            "    \"<|vi|>\": 50278,\n",
            "    \"<|yi|>\": 50335,\n",
            "    \"<|yo|>\": 50325,\n",
            "    \"<|zh|>\": 50260\n",
            "  },\n",
            "  \"language\": \"<|en|>\",\n",
            "  \"max_initial_timestamp_index\": 50,\n",
            "  \"max_length\": 448,\n",
            "  \"no_timestamps_token_id\": 50363,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"prev_sot_token_id\": 50361,\n",
            "  \"return_timestamps\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"task\": \"transcribe\",\n",
            "  \"task_to_id\": {\n",
            "    \"transcribe\": 50359,\n",
            "    \"translate\": 50358\n",
            "  },\n",
            "  \"use_scan\": false\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/vocab.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/merges.txt\n",
            "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/normalizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer_config.json\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/preprocessor_config.json\n",
            "Feature extractor WhisperFeatureExtractor {\n",
            "  \"chunk_length\": 30,\n",
            "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
            "  \"feature_size\": 80,\n",
            "  \"hop_length\": 160,\n",
            "  \"n_fft\": 400,\n",
            "  \"n_samples\": 480000,\n",
            "  \"nb_max_frames\": 3000,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"WhisperProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Eval results for step (32 / 96 | Eval Loss: 4.607755661010742 | Eval clap: 0.23313942551612854 | Eval wer: 144.28571428571428 | Eval clean_wer: 155.0 | Eval noisy_word_error: 117.5 | Eval percent_clean_samples: 0.5333333333333333 |)\n",
            "Step... (34 / 96 | Loss: 25.76479721069336, Learning Rate: 5e-05)\n",
            "Step... (36 / 96 | Loss: 3.2360799312591553, Learning Rate: 5e-05)\n",
            "Step... (38 / 96 | Loss: 2.7952170372009277, Learning Rate: 5e-05)\n",
            "Step... (40 / 96 | Loss: 7.789823532104492, Learning Rate: 5e-05)\n",
            "Step... (42 / 96 | Loss: 2.831207275390625, Learning Rate: 5e-05)\n",
            "Step... (44 / 96 | Loss: 2.890505790710449, Learning Rate: 5e-05)\n",
            "Step... (46 / 96 | Loss: 3.9377145767211914, Learning Rate: 5e-05)\n",
            "Step... (48 / 96 | Loss: 15.07003402709961, Learning Rate: 5e-05)\n",
            "Train steps ... :  50% 48/96 [28:13<13:14, 16.55s/it]\n",
            "\n",
            "Evaluating - Inference ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  12% 1/8 [00:00<00:02,  2.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  25% 2/8 [00:01<00:03,  1.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  38% 3/8 [00:01<00:02,  2.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  50% 4/8 [00:02<00:02,  1.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  62% 5/8 [00:02<00:01,  1.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  75% 6/8 [00:02<00:00,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  88% 7/8 [00:03<00:00,  1.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...: 100% 8/8 [00:03<00:00,  2.07it/s]\n",
            "\n",
            "\n",
            "Evaluating - Generation ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  12% 1/8 [00:48<05:38, 48.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  25% 2/8 [02:14<07:02, 70.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  38% 3/8 [02:36<04:03, 48.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  50% 4/8 [03:57<04:05, 61.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  62% 5/8 [04:29<02:32, 50.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  75% 6/8 [04:51<01:21, 40.92s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  88% 7/8 [06:14<00:54, 54.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...: 100% 8/8 [06:29<00:00, 48.74s/it]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\n",
            "text_config is None. Initializing the ClapTextConfig with default values.\n",
            "audio_config is None. initializing the ClapAudioConfig with default values.\n",
            "Model config ClapConfig {\n",
            "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\n",
            "  \"architectures\": [\n",
            "    \"ClapModel\"\n",
            "  ],\n",
            "  \"audio_config\": {\n",
            "    \"depths\": [\n",
            "      2,\n",
            "      2,\n",
            "      12,\n",
            "      2\n",
            "    ],\n",
            "    \"hidden_size\": 1024,\n",
            "    \"model_type\": \"clap_audio_model\",\n",
            "    \"patch_embeds_hidden_size\": 128\n",
            "  },\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 14.285714285714285,\n",
            "  \"model_type\": \"clap\",\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"projection_dim\": 512,\n",
            "  \"projection_hidden_act\": \"relu\",\n",
            "  \"text_config\": {\n",
            "    \"model_type\": \"clap_text_model\"\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\"\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing ClapModel.\n",
            "\n",
            "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "Feature extractor ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\n",
            "Processor ClapProcessor:\n",
            "- feature_extractor: ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"ClapProcessor\"\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ]\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at distil-whisper/distil-large-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"is_multilingual\": true,\n",
            "  \"lang_to_id\": {\n",
            "    \"<|af|>\": 50327,\n",
            "    \"<|am|>\": 50334,\n",
            "    \"<|ar|>\": 50272,\n",
            "    \"<|as|>\": 50350,\n",
            "    \"<|az|>\": 50304,\n",
            "    \"<|ba|>\": 50355,\n",
            "    \"<|be|>\": 50330,\n",
            "    \"<|bg|>\": 50292,\n",
            "    \"<|bn|>\": 50302,\n",
            "    \"<|bo|>\": 50347,\n",
            "    \"<|br|>\": 50309,\n",
            "    \"<|bs|>\": 50315,\n",
            "    \"<|ca|>\": 50270,\n",
            "    \"<|cs|>\": 50283,\n",
            "    \"<|cy|>\": 50297,\n",
            "    \"<|da|>\": 50285,\n",
            "    \"<|de|>\": 50261,\n",
            "    \"<|el|>\": 50281,\n",
            "    \"<|en|>\": 50259,\n",
            "    \"<|es|>\": 50262,\n",
            "    \"<|et|>\": 50307,\n",
            "    \"<|eu|>\": 50310,\n",
            "    \"<|fa|>\": 50300,\n",
            "    \"<|fi|>\": 50277,\n",
            "    \"<|fo|>\": 50338,\n",
            "    \"<|fr|>\": 50265,\n",
            "    \"<|gl|>\": 50319,\n",
            "    \"<|gu|>\": 50333,\n",
            "    \"<|haw|>\": 50352,\n",
            "    \"<|ha|>\": 50354,\n",
            "    \"<|he|>\": 50279,\n",
            "    \"<|hi|>\": 50276,\n",
            "    \"<|hr|>\": 50291,\n",
            "    \"<|ht|>\": 50339,\n",
            "    \"<|hu|>\": 50286,\n",
            "    \"<|hy|>\": 50312,\n",
            "    \"<|id|>\": 50275,\n",
            "    \"<|is|>\": 50311,\n",
            "    \"<|it|>\": 50274,\n",
            "    \"<|ja|>\": 50266,\n",
            "    \"<|jw|>\": 50356,\n",
            "    \"<|ka|>\": 50329,\n",
            "    \"<|kk|>\": 50316,\n",
            "    \"<|km|>\": 50323,\n",
            "    \"<|kn|>\": 50306,\n",
            "    \"<|ko|>\": 50264,\n",
            "    \"<|la|>\": 50294,\n",
            "    \"<|lb|>\": 50345,\n",
            "    \"<|ln|>\": 50353,\n",
            "    \"<|lo|>\": 50336,\n",
            "    \"<|lt|>\": 50293,\n",
            "    \"<|lv|>\": 50301,\n",
            "    \"<|mg|>\": 50349,\n",
            "    \"<|mi|>\": 50295,\n",
            "    \"<|mk|>\": 50308,\n",
            "    \"<|ml|>\": 50296,\n",
            "    \"<|mn|>\": 50314,\n",
            "    \"<|mr|>\": 50320,\n",
            "    \"<|ms|>\": 50282,\n",
            "    \"<|mt|>\": 50343,\n",
            "    \"<|my|>\": 50346,\n",
            "    \"<|ne|>\": 50313,\n",
            "    \"<|nl|>\": 50271,\n",
            "    \"<|nn|>\": 50342,\n",
            "    \"<|no|>\": 50288,\n",
            "    \"<|oc|>\": 50328,\n",
            "    \"<|pa|>\": 50321,\n",
            "    \"<|pl|>\": 50269,\n",
            "    \"<|ps|>\": 50340,\n",
            "    \"<|pt|>\": 50267,\n",
            "    \"<|ro|>\": 50284,\n",
            "    \"<|ru|>\": 50263,\n",
            "    \"<|sa|>\": 50344,\n",
            "    \"<|sd|>\": 50332,\n",
            "    \"<|si|>\": 50322,\n",
            "    \"<|sk|>\": 50298,\n",
            "    \"<|sl|>\": 50305,\n",
            "    \"<|sn|>\": 50324,\n",
            "    \"<|so|>\": 50326,\n",
            "    \"<|sq|>\": 50317,\n",
            "    \"<|sr|>\": 50303,\n",
            "    \"<|su|>\": 50357,\n",
            "    \"<|sv|>\": 50273,\n",
            "    \"<|sw|>\": 50318,\n",
            "    \"<|ta|>\": 50287,\n",
            "    \"<|te|>\": 50299,\n",
            "    \"<|tg|>\": 50331,\n",
            "    \"<|th|>\": 50289,\n",
            "    \"<|tk|>\": 50341,\n",
            "    \"<|tl|>\": 50348,\n",
            "    \"<|tr|>\": 50268,\n",
            "    \"<|tt|>\": 50351,\n",
            "    \"<|uk|>\": 50280,\n",
            "    \"<|ur|>\": 50290,\n",
            "    \"<|uz|>\": 50337,\n",
            "    \"<|vi|>\": 50278,\n",
            "    \"<|yi|>\": 50335,\n",
            "    \"<|yo|>\": 50325,\n",
            "    \"<|zh|>\": 50260\n",
            "  },\n",
            "  \"language\": \"<|en|>\",\n",
            "  \"max_initial_timestamp_index\": 50,\n",
            "  \"max_length\": 448,\n",
            "  \"no_timestamps_token_id\": 50363,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"prev_sot_token_id\": 50361,\n",
            "  \"return_timestamps\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"task\": \"transcribe\",\n",
            "  \"task_to_id\": {\n",
            "    \"transcribe\": 50359,\n",
            "    \"translate\": 50358\n",
            "  },\n",
            "  \"use_scan\": false\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/vocab.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/merges.txt\n",
            "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/normalizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer_config.json\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/preprocessor_config.json\n",
            "Feature extractor WhisperFeatureExtractor {\n",
            "  \"chunk_length\": 30,\n",
            "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
            "  \"feature_size\": 80,\n",
            "  \"hop_length\": 160,\n",
            "  \"n_fft\": 400,\n",
            "  \"n_samples\": 480000,\n",
            "  \"nb_max_frames\": 3000,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"WhisperProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Eval results for step (48 / 96 | Eval Loss: 4.575091361999512 | Eval clap: 0.2571921646595001 | Eval wer: 153.80952380952382 | Eval clean_wer: 182.82442748091603 | Eval noisy_word_error: 105.69620253164558 | Eval percent_clean_samples: 0.6 |)\n",
            "Step... (50 / 96 | Loss: 2.7954390048980713, Learning Rate: 5e-05)\n",
            "Step... (52 / 96 | Loss: 3.1161696910858154, Learning Rate: 5e-05)\n",
            "Step... (54 / 96 | Loss: 12.2843656539917, Learning Rate: 5e-05)\n",
            "Step... (56 / 96 | Loss: 2.7757561206817627, Learning Rate: 5e-05)\n",
            "Step... (58 / 96 | Loss: 2.961115598678589, Learning Rate: 5e-05)\n",
            "Step... (60 / 96 | Loss: 4.822300434112549, Learning Rate: 5e-05)\n",
            "Step... (62 / 96 | Loss: 2.762953281402588, Learning Rate: 5e-05)\n",
            "Step... (64 / 96 | Loss: 2.6460556983947754, Learning Rate: 5e-05)\n",
            "Train steps ... :  67% 64/96 [39:44<09:42, 18.21s/it]\n",
            "\n",
            "Evaluating - Inference ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  12% 1/8 [00:00<00:02,  2.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  25% 2/8 [00:01<00:03,  1.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  38% 3/8 [00:01<00:02,  2.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  50% 4/8 [00:02<00:02,  1.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  62% 5/8 [00:02<00:01,  1.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  75% 6/8 [00:02<00:00,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  88% 7/8 [00:03<00:00,  1.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...: 100% 8/8 [00:03<00:00,  2.05it/s]\n",
            "\n",
            "\n",
            "Evaluating - Generation ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  12% 1/8 [00:41<04:52, 41.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  25% 2/8 [02:09<06:51, 68.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  38% 3/8 [02:33<04:01, 48.24s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  50% 4/8 [03:47<03:53, 58.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  62% 5/8 [04:23<02:30, 50.32s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  75% 6/8 [04:35<01:14, 37.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  88% 7/8 [05:07<00:35, 35.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...: 100% 8/8 [05:21<00:00, 40.23s/it]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\n",
            "text_config is None. Initializing the ClapTextConfig with default values.\n",
            "audio_config is None. initializing the ClapAudioConfig with default values.\n",
            "Model config ClapConfig {\n",
            "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\n",
            "  \"architectures\": [\n",
            "    \"ClapModel\"\n",
            "  ],\n",
            "  \"audio_config\": {\n",
            "    \"depths\": [\n",
            "      2,\n",
            "      2,\n",
            "      12,\n",
            "      2\n",
            "    ],\n",
            "    \"hidden_size\": 1024,\n",
            "    \"model_type\": \"clap_audio_model\",\n",
            "    \"patch_embeds_hidden_size\": 128\n",
            "  },\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 14.285714285714285,\n",
            "  \"model_type\": \"clap\",\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"projection_dim\": 512,\n",
            "  \"projection_hidden_act\": \"relu\",\n",
            "  \"text_config\": {\n",
            "    \"model_type\": \"clap_text_model\"\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\"\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing ClapModel.\n",
            "\n",
            "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "Feature extractor ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\n",
            "Processor ClapProcessor:\n",
            "- feature_extractor: ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"ClapProcessor\"\n",
            "}\n",
            "\n",
            "\n",
            "model.safetensors:   0% 0.00/776M [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   3% 21.0M/776M [00:00<00:07, 105MB/s]\u001b[A\n",
            "model.safetensors:   7% 52.4M/776M [00:00<00:04, 181MB/s]\u001b[A\n",
            "model.safetensors:  11% 83.9M/776M [00:00<00:03, 218MB/s]\u001b[A\n",
            "model.safetensors:  15% 115M/776M [00:00<00:02, 230MB/s] \u001b[A\n",
            "model.safetensors:  19% 147M/776M [00:00<00:02, 233MB/s]\u001b[A\n",
            "model.safetensors:  23% 178M/776M [00:00<00:02, 232MB/s]\u001b[A\n",
            "model.safetensors:  27% 210M/776M [00:03<00:17, 31.9MB/s]\u001b[A\n",
            "model.safetensors:  30% 231M/776M [00:03<00:14, 37.9MB/s]\u001b[A\n",
            "model.safetensors:  32% 252M/776M [00:03<00:11, 44.7MB/s]\u001b[A\n",
            "model.safetensors:  35% 273M/776M [00:04<00:09, 53.1MB/s]\u001b[A\n",
            "model.safetensors:  38% 294M/776M [00:04<00:07, 61.8MB/s]\u001b[A\n",
            "model.safetensors:  41% 315M/776M [00:04<00:06, 72.0MB/s]\u001b[A\n",
            "model.safetensors:  43% 336M/776M [00:04<00:05, 82.8MB/s]\u001b[A\n",
            "model.safetensors:  46% 357M/776M [00:04<00:04, 92.4MB/s]\u001b[A\n",
            "model.safetensors:  49% 377M/776M [00:04<00:03, 105MB/s] \u001b[A\n",
            "model.safetensors:  51% 398M/776M [00:05<00:06, 62.6MB/s]\u001b[A\n",
            "model.safetensors:  54% 419M/776M [00:05<00:04, 74.6MB/s]\u001b[A\n",
            "model.safetensors:  57% 440M/776M [00:06<00:07, 46.8MB/s]\u001b[A\n",
            "model.safetensors:  59% 461M/776M [00:06<00:05, 60.1MB/s]\u001b[A\n",
            "model.safetensors:  62% 482M/776M [00:06<00:03, 76.4MB/s]\u001b[A\n",
            "model.safetensors:  65% 503M/776M [00:06<00:02, 93.4MB/s]\u001b[A\n",
            "model.safetensors:  68% 524M/776M [00:07<00:02, 112MB/s] \u001b[A\n",
            "model.safetensors:  72% 556M/776M [00:08<00:03, 58.5MB/s]\u001b[A\n",
            "model.safetensors:  77% 598M/776M [00:08<00:01, 89.3MB/s]\u001b[A\n",
            "model.safetensors:  80% 619M/776M [00:08<00:01, 98.9MB/s]\u001b[A\n",
            "model.safetensors:  82% 640M/776M [00:08<00:01, 114MB/s] \u001b[A\n",
            "model.safetensors:  86% 671M/776M [00:08<00:00, 137MB/s]\u001b[Aloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "\n",
            "model.safetensors:  90% 703M/776M [00:08<00:00, 158MB/s]\u001b[Aloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ]\n",
            "}\n",
            "\n",
            "\n",
            "model.safetensors:  93% 724M/776M [00:09<00:00, 85.9MB/s]\u001b[A\n",
            "model.safetensors:  96% 744M/776M [00:09<00:00, 92.0MB/s]\u001b[A\n",
            "model.safetensors: 100% 776M/776M [00:09<00:00, 80.7MB/s]\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at distil-whisper/distil-large-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"is_multilingual\": true,\n",
            "  \"lang_to_id\": {\n",
            "    \"<|af|>\": 50327,\n",
            "    \"<|am|>\": 50334,\n",
            "    \"<|ar|>\": 50272,\n",
            "    \"<|as|>\": 50350,\n",
            "    \"<|az|>\": 50304,\n",
            "    \"<|ba|>\": 50355,\n",
            "    \"<|be|>\": 50330,\n",
            "    \"<|bg|>\": 50292,\n",
            "    \"<|bn|>\": 50302,\n",
            "    \"<|bo|>\": 50347,\n",
            "    \"<|br|>\": 50309,\n",
            "    \"<|bs|>\": 50315,\n",
            "    \"<|ca|>\": 50270,\n",
            "    \"<|cs|>\": 50283,\n",
            "    \"<|cy|>\": 50297,\n",
            "    \"<|da|>\": 50285,\n",
            "    \"<|de|>\": 50261,\n",
            "    \"<|el|>\": 50281,\n",
            "    \"<|en|>\": 50259,\n",
            "    \"<|es|>\": 50262,\n",
            "    \"<|et|>\": 50307,\n",
            "    \"<|eu|>\": 50310,\n",
            "    \"<|fa|>\": 50300,\n",
            "    \"<|fi|>\": 50277,\n",
            "    \"<|fo|>\": 50338,\n",
            "    \"<|fr|>\": 50265,\n",
            "    \"<|gl|>\": 50319,\n",
            "    \"<|gu|>\": 50333,\n",
            "    \"<|haw|>\": 50352,\n",
            "    \"<|ha|>\": 50354,\n",
            "    \"<|he|>\": 50279,\n",
            "    \"<|hi|>\": 50276,\n",
            "    \"<|hr|>\": 50291,\n",
            "    \"<|ht|>\": 50339,\n",
            "    \"<|hu|>\": 50286,\n",
            "    \"<|hy|>\": 50312,\n",
            "    \"<|id|>\": 50275,\n",
            "    \"<|is|>\": 50311,\n",
            "    \"<|it|>\": 50274,\n",
            "    \"<|ja|>\": 50266,\n",
            "    \"<|jw|>\": 50356,\n",
            "    \"<|ka|>\": 50329,\n",
            "    \"<|kk|>\": 50316,\n",
            "    \"<|km|>\": 50323,\n",
            "    \"<|kn|>\": 50306,\n",
            "    \"<|ko|>\": 50264,\n",
            "    \"<|la|>\": 50294,\n",
            "    \"<|lb|>\": 50345,\n",
            "    \"<|ln|>\": 50353,\n",
            "    \"<|lo|>\": 50336,\n",
            "    \"<|lt|>\": 50293,\n",
            "    \"<|lv|>\": 50301,\n",
            "    \"<|mg|>\": 50349,\n",
            "    \"<|mi|>\": 50295,\n",
            "    \"<|mk|>\": 50308,\n",
            "    \"<|ml|>\": 50296,\n",
            "    \"<|mn|>\": 50314,\n",
            "    \"<|mr|>\": 50320,\n",
            "    \"<|ms|>\": 50282,\n",
            "    \"<|mt|>\": 50343,\n",
            "    \"<|my|>\": 50346,\n",
            "    \"<|ne|>\": 50313,\n",
            "    \"<|nl|>\": 50271,\n",
            "    \"<|nn|>\": 50342,\n",
            "    \"<|no|>\": 50288,\n",
            "    \"<|oc|>\": 50328,\n",
            "    \"<|pa|>\": 50321,\n",
            "    \"<|pl|>\": 50269,\n",
            "    \"<|ps|>\": 50340,\n",
            "    \"<|pt|>\": 50267,\n",
            "    \"<|ro|>\": 50284,\n",
            "    \"<|ru|>\": 50263,\n",
            "    \"<|sa|>\": 50344,\n",
            "    \"<|sd|>\": 50332,\n",
            "    \"<|si|>\": 50322,\n",
            "    \"<|sk|>\": 50298,\n",
            "    \"<|sl|>\": 50305,\n",
            "    \"<|sn|>\": 50324,\n",
            "    \"<|so|>\": 50326,\n",
            "    \"<|sq|>\": 50317,\n",
            "    \"<|sr|>\": 50303,\n",
            "    \"<|su|>\": 50357,\n",
            "    \"<|sv|>\": 50273,\n",
            "    \"<|sw|>\": 50318,\n",
            "    \"<|ta|>\": 50287,\n",
            "    \"<|te|>\": 50299,\n",
            "    \"<|tg|>\": 50331,\n",
            "    \"<|th|>\": 50289,\n",
            "    \"<|tk|>\": 50341,\n",
            "    \"<|tl|>\": 50348,\n",
            "    \"<|tr|>\": 50268,\n",
            "    \"<|tt|>\": 50351,\n",
            "    \"<|uk|>\": 50280,\n",
            "    \"<|ur|>\": 50290,\n",
            "    \"<|uz|>\": 50337,\n",
            "    \"<|vi|>\": 50278,\n",
            "    \"<|yi|>\": 50335,\n",
            "    \"<|yo|>\": 50325,\n",
            "    \"<|zh|>\": 50260\n",
            "  },\n",
            "  \"language\": \"<|en|>\",\n",
            "  \"max_initial_timestamp_index\": 50,\n",
            "  \"max_length\": 448,\n",
            "  \"no_timestamps_token_id\": 50363,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"prev_sot_token_id\": 50361,\n",
            "  \"return_timestamps\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"task\": \"transcribe\",\n",
            "  \"task_to_id\": {\n",
            "    \"transcribe\": 50359,\n",
            "    \"translate\": 50358\n",
            "  },\n",
            "  \"use_scan\": false\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/vocab.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/merges.txt\n",
            "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/normalizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer_config.json\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/preprocessor_config.json\n",
            "Feature extractor WhisperFeatureExtractor {\n",
            "  \"chunk_length\": 30,\n",
            "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
            "  \"feature_size\": 80,\n",
            "  \"hop_length\": 160,\n",
            "  \"n_fft\": 400,\n",
            "  \"n_samples\": 480000,\n",
            "  \"nb_max_frames\": 3000,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"WhisperProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Eval results for step (64 / 96 | Eval Loss: 4.557331085205078 | Eval clap: 0.21902552247047424 | Eval wer: 137.38095238095238 | Eval clean_wer: 143.73259052924791 | Eval noisy_word_error: 100.0 | Eval percent_clean_samples: 0.7333333333333333 |)\n",
            "Step... (66 / 96 | Loss: 2.673461437225342, Learning Rate: 5e-05)\n",
            "Step... (68 / 96 | Loss: 24.42303466796875, Learning Rate: 5e-05)\n",
            "Step... (70 / 96 | Loss: 2.741948127746582, Learning Rate: 5e-05)\n",
            "Step... (72 / 96 | Loss: 2.6086719036102295, Learning Rate: 5e-05)\n",
            "Step... (74 / 96 | Loss: 7.241959095001221, Learning Rate: 5e-05)\n",
            "Step... (76 / 96 | Loss: 2.990563154220581, Learning Rate: 5e-05)\n",
            "Step... (78 / 96 | Loss: 2.773768901824951, Learning Rate: 5e-05)\n",
            "Step... (80 / 96 | Loss: 3.399160385131836, Learning Rate: 5e-05)\n",
            "Train steps ... :  83% 80/96 [50:09<05:25, 20.34s/it]\n",
            "\n",
            "Evaluating - Inference ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  12% 1/8 [00:00<00:02,  2.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  25% 2/8 [00:01<00:03,  1.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  38% 3/8 [00:01<00:02,  2.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  50% 4/8 [00:02<00:02,  1.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  62% 5/8 [00:02<00:01,  1.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  75% 6/8 [00:02<00:00,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  88% 7/8 [00:03<00:00,  1.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...: 100% 8/8 [00:03<00:00,  2.02it/s]\n",
            "\n",
            "\n",
            "Evaluating - Generation ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  12% 1/8 [00:32<03:48, 32.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  25% 2/8 [01:48<05:48, 58.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  38% 3/8 [02:22<03:54, 46.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  50% 4/8 [03:28<03:38, 54.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  62% 5/8 [04:00<02:19, 46.36s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  75% 6/8 [04:17<01:13, 36.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  88% 7/8 [05:14<00:43, 43.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...: 100% 8/8 [05:25<00:00, 40.74s/it]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\n",
            "text_config is None. Initializing the ClapTextConfig with default values.\n",
            "audio_config is None. initializing the ClapAudioConfig with default values.\n",
            "Model config ClapConfig {\n",
            "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\n",
            "  \"architectures\": [\n",
            "    \"ClapModel\"\n",
            "  ],\n",
            "  \"audio_config\": {\n",
            "    \"depths\": [\n",
            "      2,\n",
            "      2,\n",
            "      12,\n",
            "      2\n",
            "    ],\n",
            "    \"hidden_size\": 1024,\n",
            "    \"model_type\": \"clap_audio_model\",\n",
            "    \"patch_embeds_hidden_size\": 128\n",
            "  },\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 14.285714285714285,\n",
            "  \"model_type\": \"clap\",\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"projection_dim\": 512,\n",
            "  \"projection_hidden_act\": \"relu\",\n",
            "  \"text_config\": {\n",
            "    \"model_type\": \"clap_text_model\"\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\"\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing ClapModel.\n",
            "\n",
            "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "Feature extractor ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\n",
            "Processor ClapProcessor:\n",
            "- feature_extractor: ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"ClapProcessor\"\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ]\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at distil-whisper/distil-large-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"is_multilingual\": true,\n",
            "  \"lang_to_id\": {\n",
            "    \"<|af|>\": 50327,\n",
            "    \"<|am|>\": 50334,\n",
            "    \"<|ar|>\": 50272,\n",
            "    \"<|as|>\": 50350,\n",
            "    \"<|az|>\": 50304,\n",
            "    \"<|ba|>\": 50355,\n",
            "    \"<|be|>\": 50330,\n",
            "    \"<|bg|>\": 50292,\n",
            "    \"<|bn|>\": 50302,\n",
            "    \"<|bo|>\": 50347,\n",
            "    \"<|br|>\": 50309,\n",
            "    \"<|bs|>\": 50315,\n",
            "    \"<|ca|>\": 50270,\n",
            "    \"<|cs|>\": 50283,\n",
            "    \"<|cy|>\": 50297,\n",
            "    \"<|da|>\": 50285,\n",
            "    \"<|de|>\": 50261,\n",
            "    \"<|el|>\": 50281,\n",
            "    \"<|en|>\": 50259,\n",
            "    \"<|es|>\": 50262,\n",
            "    \"<|et|>\": 50307,\n",
            "    \"<|eu|>\": 50310,\n",
            "    \"<|fa|>\": 50300,\n",
            "    \"<|fi|>\": 50277,\n",
            "    \"<|fo|>\": 50338,\n",
            "    \"<|fr|>\": 50265,\n",
            "    \"<|gl|>\": 50319,\n",
            "    \"<|gu|>\": 50333,\n",
            "    \"<|haw|>\": 50352,\n",
            "    \"<|ha|>\": 50354,\n",
            "    \"<|he|>\": 50279,\n",
            "    \"<|hi|>\": 50276,\n",
            "    \"<|hr|>\": 50291,\n",
            "    \"<|ht|>\": 50339,\n",
            "    \"<|hu|>\": 50286,\n",
            "    \"<|hy|>\": 50312,\n",
            "    \"<|id|>\": 50275,\n",
            "    \"<|is|>\": 50311,\n",
            "    \"<|it|>\": 50274,\n",
            "    \"<|ja|>\": 50266,\n",
            "    \"<|jw|>\": 50356,\n",
            "    \"<|ka|>\": 50329,\n",
            "    \"<|kk|>\": 50316,\n",
            "    \"<|km|>\": 50323,\n",
            "    \"<|kn|>\": 50306,\n",
            "    \"<|ko|>\": 50264,\n",
            "    \"<|la|>\": 50294,\n",
            "    \"<|lb|>\": 50345,\n",
            "    \"<|ln|>\": 50353,\n",
            "    \"<|lo|>\": 50336,\n",
            "    \"<|lt|>\": 50293,\n",
            "    \"<|lv|>\": 50301,\n",
            "    \"<|mg|>\": 50349,\n",
            "    \"<|mi|>\": 50295,\n",
            "    \"<|mk|>\": 50308,\n",
            "    \"<|ml|>\": 50296,\n",
            "    \"<|mn|>\": 50314,\n",
            "    \"<|mr|>\": 50320,\n",
            "    \"<|ms|>\": 50282,\n",
            "    \"<|mt|>\": 50343,\n",
            "    \"<|my|>\": 50346,\n",
            "    \"<|ne|>\": 50313,\n",
            "    \"<|nl|>\": 50271,\n",
            "    \"<|nn|>\": 50342,\n",
            "    \"<|no|>\": 50288,\n",
            "    \"<|oc|>\": 50328,\n",
            "    \"<|pa|>\": 50321,\n",
            "    \"<|pl|>\": 50269,\n",
            "    \"<|ps|>\": 50340,\n",
            "    \"<|pt|>\": 50267,\n",
            "    \"<|ro|>\": 50284,\n",
            "    \"<|ru|>\": 50263,\n",
            "    \"<|sa|>\": 50344,\n",
            "    \"<|sd|>\": 50332,\n",
            "    \"<|si|>\": 50322,\n",
            "    \"<|sk|>\": 50298,\n",
            "    \"<|sl|>\": 50305,\n",
            "    \"<|sn|>\": 50324,\n",
            "    \"<|so|>\": 50326,\n",
            "    \"<|sq|>\": 50317,\n",
            "    \"<|sr|>\": 50303,\n",
            "    \"<|su|>\": 50357,\n",
            "    \"<|sv|>\": 50273,\n",
            "    \"<|sw|>\": 50318,\n",
            "    \"<|ta|>\": 50287,\n",
            "    \"<|te|>\": 50299,\n",
            "    \"<|tg|>\": 50331,\n",
            "    \"<|th|>\": 50289,\n",
            "    \"<|tk|>\": 50341,\n",
            "    \"<|tl|>\": 50348,\n",
            "    \"<|tr|>\": 50268,\n",
            "    \"<|tt|>\": 50351,\n",
            "    \"<|uk|>\": 50280,\n",
            "    \"<|ur|>\": 50290,\n",
            "    \"<|uz|>\": 50337,\n",
            "    \"<|vi|>\": 50278,\n",
            "    \"<|yi|>\": 50335,\n",
            "    \"<|yo|>\": 50325,\n",
            "    \"<|zh|>\": 50260\n",
            "  },\n",
            "  \"language\": \"<|en|>\",\n",
            "  \"max_initial_timestamp_index\": 50,\n",
            "  \"max_length\": 448,\n",
            "  \"no_timestamps_token_id\": 50363,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"prev_sot_token_id\": 50361,\n",
            "  \"return_timestamps\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"task\": \"transcribe\",\n",
            "  \"task_to_id\": {\n",
            "    \"transcribe\": 50359,\n",
            "    \"translate\": 50358\n",
            "  },\n",
            "  \"use_scan\": false\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/vocab.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/merges.txt\n",
            "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/normalizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer_config.json\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/preprocessor_config.json\n",
            "Feature extractor WhisperFeatureExtractor {\n",
            "  \"chunk_length\": 30,\n",
            "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
            "  \"feature_size\": 80,\n",
            "  \"hop_length\": 160,\n",
            "  \"n_fft\": 400,\n",
            "  \"n_samples\": 480000,\n",
            "  \"nb_max_frames\": 3000,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"WhisperProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Eval results for step (80 / 96 | Eval Loss: 4.546652793884277 | Eval clap: 0.2524122893810272 | Eval wer: 120.23809523809523 | Eval clean_wer: 126.10441767068272 | Eval noisy_word_error: 111.69590643274854 | Eval percent_clean_samples: 0.5333333333333333 |)\n",
            "Step... (82 / 96 | Loss: 16.842941284179688, Learning Rate: 5e-05)\n",
            "Step... (84 / 96 | Loss: 2.012234687805176, Learning Rate: 5e-05)\n",
            "Step... (86 / 96 | Loss: 3.1443967819213867, Learning Rate: 5e-05)\n",
            "Step... (88 / 96 | Loss: 12.631525993347168, Learning Rate: 5e-05)\n",
            "Step... (90 / 96 | Loss: 3.080404758453369, Learning Rate: 5e-05)\n",
            "Step... (92 / 96 | Loss: 3.0590171813964844, Learning Rate: 5e-05)\n",
            "Step... (94 / 96 | Loss: 4.34886360168457, Learning Rate: 5e-05)\n",
            "Step... (96 / 96 | Loss: 2.594143867492676, Learning Rate: 5e-05)\n",
            "Train steps ... : 100% 96/96 [1:00:16<00:00, 13.33s/it]Configuration saved in ./output_dir_training/config.json\n",
            "Configuration saved in ./output_dir_training/generation_config.json\n",
            "Model weights saved in ./output_dir_training/model.safetensors\n",
            "\n",
            "\n",
            "Evaluating - Inference ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  12% 1/8 [00:00<00:03,  2.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  25% 2/8 [00:01<00:04,  1.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  38% 3/8 [00:01<00:02,  1.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  50% 4/8 [00:02<00:02,  1.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  62% 5/8 [00:02<00:01,  1.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  75% 6/8 [00:03<00:00,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...:  88% 7/8 [00:04<00:00,  1.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Inference ...: 100% 8/8 [00:04<00:00,  1.93it/s]\n",
            "\n",
            "\n",
            "Evaluating - Generation ...:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  12% 1/8 [00:46<05:24, 46.30s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  25% 2/8 [01:50<05:42, 57.00s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  38% 3/8 [02:15<03:32, 42.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  50% 4/8 [03:13<03:13, 48.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  62% 5/8 [03:46<02:09, 43.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  75% 6/8 [04:03<01:08, 34.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...:  88% 7/8 [05:20<00:48, 48.18s/it]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating - Generation ...: 100% 8/8 [05:23<00:00, 40.47s/it]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\n",
            "text_config is None. Initializing the ClapTextConfig with default values.\n",
            "audio_config is None. initializing the ClapAudioConfig with default values.\n",
            "Model config ClapConfig {\n",
            "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\n",
            "  \"architectures\": [\n",
            "    \"ClapModel\"\n",
            "  ],\n",
            "  \"audio_config\": {\n",
            "    \"depths\": [\n",
            "      2,\n",
            "      2,\n",
            "      12,\n",
            "      2\n",
            "    ],\n",
            "    \"hidden_size\": 1024,\n",
            "    \"model_type\": \"clap_audio_model\",\n",
            "    \"patch_embeds_hidden_size\": 128\n",
            "  },\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 14.285714285714285,\n",
            "  \"model_type\": \"clap\",\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"projection_dim\": 512,\n",
            "  \"projection_hidden_act\": \"relu\",\n",
            "  \"text_config\": {\n",
            "    \"model_type\": \"clap_text_model\"\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\"\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing ClapModel.\n",
            "\n",
            "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\n",
            "Feature extractor ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\n",
            "Processor ClapProcessor:\n",
            "- feature_extractor: ClapFeatureExtractor {\n",
            "  \"chunk_length_s\": 10,\n",
            "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\n",
            "  \"feature_size\": 64,\n",
            "  \"fft_window_size\": 1024,\n",
            "  \"frequency_max\": 14000,\n",
            "  \"frequency_min\": 50,\n",
            "  \"hop_length\": 480,\n",
            "  \"max_length_s\": 10,\n",
            "  \"n_fft\": 1024,\n",
            "  \"nb_frequency_bins\": 513,\n",
            "  \"nb_max_frames\": 1000,\n",
            "  \"nb_max_samples\": 480000,\n",
            "  \"padding\": \"repeatpad\",\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"ClapProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 48000,\n",
            "  \"top_db\": null,\n",
            "  \"truncation\": \"rand_trunc\"\n",
            "}\n",
            "\n",
            "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"ClapProcessor\"\n",
            "}\n",
            "\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"distil-whisper/distil-large-v2\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"apply_spec_augment\": false,\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"d_model\": 1280,\n",
            "  \"decoder_attention_heads\": 20,\n",
            "  \"decoder_ffn_dim\": 5120,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 2,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 20,\n",
            "  \"encoder_ffn_dim\": 5120,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 32,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"median_filter_width\": 7,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers.js_config\": {\n",
            "    \"use_external_data_format\": {\n",
            "      \"encoder_model.onnx\": true\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ]\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at distil-whisper/distil-large-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"is_multilingual\": true,\n",
            "  \"lang_to_id\": {\n",
            "    \"<|af|>\": 50327,\n",
            "    \"<|am|>\": 50334,\n",
            "    \"<|ar|>\": 50272,\n",
            "    \"<|as|>\": 50350,\n",
            "    \"<|az|>\": 50304,\n",
            "    \"<|ba|>\": 50355,\n",
            "    \"<|be|>\": 50330,\n",
            "    \"<|bg|>\": 50292,\n",
            "    \"<|bn|>\": 50302,\n",
            "    \"<|bo|>\": 50347,\n",
            "    \"<|br|>\": 50309,\n",
            "    \"<|bs|>\": 50315,\n",
            "    \"<|ca|>\": 50270,\n",
            "    \"<|cs|>\": 50283,\n",
            "    \"<|cy|>\": 50297,\n",
            "    \"<|da|>\": 50285,\n",
            "    \"<|de|>\": 50261,\n",
            "    \"<|el|>\": 50281,\n",
            "    \"<|en|>\": 50259,\n",
            "    \"<|es|>\": 50262,\n",
            "    \"<|et|>\": 50307,\n",
            "    \"<|eu|>\": 50310,\n",
            "    \"<|fa|>\": 50300,\n",
            "    \"<|fi|>\": 50277,\n",
            "    \"<|fo|>\": 50338,\n",
            "    \"<|fr|>\": 50265,\n",
            "    \"<|gl|>\": 50319,\n",
            "    \"<|gu|>\": 50333,\n",
            "    \"<|haw|>\": 50352,\n",
            "    \"<|ha|>\": 50354,\n",
            "    \"<|he|>\": 50279,\n",
            "    \"<|hi|>\": 50276,\n",
            "    \"<|hr|>\": 50291,\n",
            "    \"<|ht|>\": 50339,\n",
            "    \"<|hu|>\": 50286,\n",
            "    \"<|hy|>\": 50312,\n",
            "    \"<|id|>\": 50275,\n",
            "    \"<|is|>\": 50311,\n",
            "    \"<|it|>\": 50274,\n",
            "    \"<|ja|>\": 50266,\n",
            "    \"<|jw|>\": 50356,\n",
            "    \"<|ka|>\": 50329,\n",
            "    \"<|kk|>\": 50316,\n",
            "    \"<|km|>\": 50323,\n",
            "    \"<|kn|>\": 50306,\n",
            "    \"<|ko|>\": 50264,\n",
            "    \"<|la|>\": 50294,\n",
            "    \"<|lb|>\": 50345,\n",
            "    \"<|ln|>\": 50353,\n",
            "    \"<|lo|>\": 50336,\n",
            "    \"<|lt|>\": 50293,\n",
            "    \"<|lv|>\": 50301,\n",
            "    \"<|mg|>\": 50349,\n",
            "    \"<|mi|>\": 50295,\n",
            "    \"<|mk|>\": 50308,\n",
            "    \"<|ml|>\": 50296,\n",
            "    \"<|mn|>\": 50314,\n",
            "    \"<|mr|>\": 50320,\n",
            "    \"<|ms|>\": 50282,\n",
            "    \"<|mt|>\": 50343,\n",
            "    \"<|my|>\": 50346,\n",
            "    \"<|ne|>\": 50313,\n",
            "    \"<|nl|>\": 50271,\n",
            "    \"<|nn|>\": 50342,\n",
            "    \"<|no|>\": 50288,\n",
            "    \"<|oc|>\": 50328,\n",
            "    \"<|pa|>\": 50321,\n",
            "    \"<|pl|>\": 50269,\n",
            "    \"<|ps|>\": 50340,\n",
            "    \"<|pt|>\": 50267,\n",
            "    \"<|ro|>\": 50284,\n",
            "    \"<|ru|>\": 50263,\n",
            "    \"<|sa|>\": 50344,\n",
            "    \"<|sd|>\": 50332,\n",
            "    \"<|si|>\": 50322,\n",
            "    \"<|sk|>\": 50298,\n",
            "    \"<|sl|>\": 50305,\n",
            "    \"<|sn|>\": 50324,\n",
            "    \"<|so|>\": 50326,\n",
            "    \"<|sq|>\": 50317,\n",
            "    \"<|sr|>\": 50303,\n",
            "    \"<|su|>\": 50357,\n",
            "    \"<|sv|>\": 50273,\n",
            "    \"<|sw|>\": 50318,\n",
            "    \"<|ta|>\": 50287,\n",
            "    \"<|te|>\": 50299,\n",
            "    \"<|tg|>\": 50331,\n",
            "    \"<|th|>\": 50289,\n",
            "    \"<|tk|>\": 50341,\n",
            "    \"<|tl|>\": 50348,\n",
            "    \"<|tr|>\": 50268,\n",
            "    \"<|tt|>\": 50351,\n",
            "    \"<|uk|>\": 50280,\n",
            "    \"<|ur|>\": 50290,\n",
            "    \"<|uz|>\": 50337,\n",
            "    \"<|vi|>\": 50278,\n",
            "    \"<|yi|>\": 50335,\n",
            "    \"<|yo|>\": 50325,\n",
            "    \"<|zh|>\": 50260\n",
            "  },\n",
            "  \"language\": \"<|en|>\",\n",
            "  \"max_initial_timestamp_index\": 50,\n",
            "  \"max_length\": 448,\n",
            "  \"no_timestamps_token_id\": 50363,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"prev_sot_token_id\": 50361,\n",
            "  \"return_timestamps\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"task\": \"transcribe\",\n",
            "  \"task_to_id\": {\n",
            "    \"transcribe\": 50359,\n",
            "    \"translate\": 50358\n",
            "  },\n",
            "  \"use_scan\": false\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/vocab.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/merges.txt\n",
            "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/normalizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/tokenizer_config.json\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--distil-whisper--distil-large-v2/snapshots/ccb3693e5247bdd734c99eeab96688f8cda3ba72/preprocessor_config.json\n",
            "Feature extractor WhisperFeatureExtractor {\n",
            "  \"chunk_length\": 30,\n",
            "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
            "  \"feature_size\": 80,\n",
            "  \"hop_length\": 160,\n",
            "  \"n_fft\": 400,\n",
            "  \"n_samples\": 480000,\n",
            "  \"nb_max_frames\": 3000,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"processor_class\": \"WhisperProcessor\",\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Increase max_length from 448 to 448 since input is conditioned on previous segment.\n",
            "Eval results for step (96 / 96 | Eval Loss: 4.537570953369141 | Eval clap: 0.21992290019989014 | Eval wer: 100.95238095238095 | Eval clean_wer: 101.24223602484473 | Eval noisy_word_error: 100.0 | Eval percent_clean_samples: 0.8666666666666667 |)\n",
            "Train steps ... : 100% 96/96 [1:10:28<00:00, 13.33s/it]\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_0_loss █▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_1_loss █▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_2_loss █▅▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_3_loss █▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_4_loss █▅▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_5_loss █▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_6_loss █▅▃▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_7_loss █▅▃▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_8_loss █▅▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_0_loss ▁▁▂▁▁▅▁▁▄▁▂▁▁▁█▁▂▁▁▁▁▁▃▁▁▁▁▁▇▁▂▁▁▁▄▁▃▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_1_loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁█▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_2_loss ▁▁▃▁▁▆▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁█▁▃▁▁▁▆▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_3_loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁█▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_4_loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁▇▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_5_loss ▁▁▃▁▁▆▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁█▁▂▁▁▁▆▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_6_loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁▇▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_7_loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁▇▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_8_loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁█▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/clap █▂▅▁▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/clean_wer ▆▆█▅▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/noisy_word_error █▅▂▁▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/percent_clean_samples ▁▃▄▆▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/time █▆▇▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       eval/wer ▆▇█▆▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▄▄▄▄▄▄▅▅▅▅▅▅▅▅▇▇▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/grad_norm █▅▄▃▂▂▂▂▂▂▂▂▁▁▆▁▁▁▁▁▁▁▂▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁▂▃▄▅▇██████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁▁▃▁▁▅▁▁▄▁▂▁▁▁█▁▃▁▁▁▁▁▄▁▁▁▁▁█▁▂▁▁▁▅▁▄▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/time ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_0_loss 2.80719\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_1_loss 4.14466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_2_loss 4.75508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_3_loss 4.85944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_4_loss 4.85092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_5_loss 4.83617\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_6_loss 4.8522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_7_loss 4.8631\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  codebook_eval/codebook_8_loss 4.86938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_0_loss 1.78029\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_1_loss 2.51307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_2_loss 2.71116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_3_loss 2.75011\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_4_loss 2.72867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_5_loss 2.79839\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_6_loss 2.68182\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_7_loss 2.69445\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: codebook_train/codebook_8_loss 2.68934\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/clap 0.21992\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/clean_wer 101.24224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/epoch 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 4.53757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/noisy_word_error 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/percent_clean_samples 0.86667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/time 328.45814\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       eval/wer 100.95238\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/grad_norm 0.27268\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 2.59414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/time 1554.79038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstoic-wildflower-42\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/sivakgp-iit-kharagpur/parler-speech/runs/g990r8zh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/sivakgp-iit-kharagpur/parler-speech\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 96 media file(s), 12 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250321_062207-g990r8zh/logs\u001b[0m\n",
            "Train steps ... : 100% 96/96 [1:10:29<00:00, 44.06s/it]\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch ./training/run_parler_tts_training.py \\\n",
        "    --model_name_or_path \"ai4bharat/indic-parler-tts\" \\\n",
        "    --feature_extractor_name \"ylacombe/dac_44khZ\" \\\n",
        "    --description_tokenizer_name \"google/flan-t5-large\" \\\n",
        "    --prompt_tokenizer_name \"ai4bharat/indic-parler-tts\" \\\n",
        "    --report_to \"wandb\" \\\n",
        "    --overwrite_output_dir true \\\n",
        "    --train_dataset_name \"sivakgp/hindi-books-audio-train-validation\" \\\n",
        "    --train_metadata_dataset_name \"sivakgp/hindi-books-audio-train-validation-description\" \\\n",
        "    --train_dataset_config_name \"default\" \\\n",
        "    --train_split_name \"train\" \\\n",
        "    --eval_dataset_name \"sivakgp/hindi-books-audio-train-validation\" \\\n",
        "    --eval_metadata_dataset_name \"sivakgp/hindi-books-audio-train-validation-description\" \\\n",
        "    --eval_dataset_config_name \"default\" \\\n",
        "    --eval_split_name \"validation\" \\\n",
        "    --max_eval_samples 20 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --target_audio_column_name \"audio\" \\\n",
        "    --description_column_name \"text_description\" \\\n",
        "    --prompt_column_name \"text\" \\\n",
        "    --max_duration_in_seconds 30 \\\n",
        "    --min_duration_in_seconds 2.0 \\\n",
        "    --max_text_length 400 \\\n",
        "    --preprocessing_num_workers 2 \\\n",
        "    --do_train true \\\n",
        "    --num_train_epochs 6 \\\n",
        "    --gradient_accumulation_steps 18 \\\n",
        "    --gradient_checkpointing true \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --learning_rate 0.00005 \\\n",
        "    --adam_beta1 0.9 \\\n",
        "    --adam_beta2 0.99 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --lr_scheduler_type \"constant_with_warmup\" \\\n",
        "    --warmup_steps 15 \\\n",
        "    --logging_steps 2 \\\n",
        "    --freeze_text_encoder true \\\n",
        "    --audio_encoder_per_device_batch_size 2 \\\n",
        "    --dtype \"float16\" \\\n",
        "    --seed 456 \\\n",
        "    --output_dir \"./output_dir_training/\" \\\n",
        "    --temporary_save_to_disk \"./audio_code_tmp/\" \\\n",
        "    --save_to_disk \"./tmp_dataset_audio/\" \\\n",
        "    --dataloader_num_workers 2 \\\n",
        "    --do_eval true\\\n",
        "    --predict_with_generate \\\n",
        "    --include_inputs_for_metrics \\\n",
        "    --group_by_length true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59Tobapz3wou"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "owId9zsCRBv2",
        "outputId": "6014711d-6a8f-469d-d17c-3d47616b6681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-03-16 15:15:30.186319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742138130.207135   19124 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742138130.213480   19124 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-16 15:15:30.236026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Flash attention 2 is not installed\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msivakgp\u001b[0m (\u001b[33msivakgp-iit-kharagpur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/parler-tts/wandb/run-20250316_151703-j2wh1yk9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMini\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sivakgp-iit-kharagpur/parler-tts-50k-hours\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sivakgp-iit-kharagpur/parler-tts-50k-hours/runs/j2wh1yk9\u001b[0m\n",
            "03/16/2025 15:17:05 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/16/2025 15:17:05 - INFO - __main__ - Training/evaluation parameters ParlerTTSTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.99,\n",
            "adam_epsilon=1e-08,\n",
            "audio_encoder_per_device_batch_size=24,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "codebook_weights=None,\n",
            "compute_clap_similarity_metric=True,\n",
            "compute_noise_level_metric=True,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=8,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "dtype=bfloat16,\n",
            "eval_accumulation_steps=None,\n",
            "eval_dataloader_num_workers=0,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_generation_steps=None,\n",
            "eval_on_start=False,\n",
            "eval_steps=10000,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=['inputs'],\n",
            "include_inputs_for_metrics=True,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.00095,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./output_dir_training/runs/Mar16_15-15-35_02e8cb878dff,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1000,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.CONSTANT_WITH_WARMUP,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "noise_level_to_compute_clean_wer=25,\n",
            "num_train_epochs=4,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./output_dir_training,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=6,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./output_dir_training,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=10000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=456,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=20000,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--ylacombe--dac_44khZ_8kbps/snapshots/db52bea859d9411e0beb44a3ea923a8731ee4197/preprocessor_config.json\n",
            "Feature extractor EncodecFeatureExtractor {\n",
            "  \"chunk_length_s\": null,\n",
            "  \"feature_extractor_type\": \"EncodecFeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"overlap\": null,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"return_attention_mask\": true,\n",
            "  \"sampling_rate\": 44100\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/spiece.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/tokenizer_config.json\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/spiece.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/tokenizer_config.json\n",
            "03/16/2025 15:17:06 - WARNING - __main__ - Disabling fast tokenizer warning: https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L3231-L3235\n",
            "Combining datasets...:   0% 0/4 [00:00<?, ?it/s]\n",
            "Resolving data files: 100% 18/18 [00:00<00:00, 26490.34it/s]\n",
            "\n",
            "Resolving data files:   0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 218.19it/s]\n",
            "\n",
            "Resolving data files: 100% 18/18 [00:00<00:00, 80316.46it/s]\n",
            "\n",
            "Resolving data files:   0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "Resolving data files: 100% 64/64 [00:00<00:00, 331.21it/s]\n",
            "Setting num_proc from 8 to 4 for the dev.clean split as it only contains 4 shards.\n",
            "03/16/2025 15:17:15 - WARNING - datasets.builder - Setting num_proc from 8 to 4 for the dev.clean split as it only contains 4 shards.\n",
            "\n",
            "Generating dev.clean split:   0% 0/5589 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating dev.clean split:   2% 100/5589 [00:01<01:06, 83.14 examples/s]\u001b[A\n",
            "Generating dev.clean split:   5% 300/5589 [00:01<00:21, 250.51 examples/s]\u001b[A\n",
            "Generating dev.clean split:   7% 400/5589 [00:01<00:16, 308.08 examples/s]\u001b[A\n",
            "Generating dev.clean split:   9% 500/5589 [00:01<00:12, 394.71 examples/s]\u001b[A\n",
            "Generating dev.clean split:  11% 600/5589 [00:01<00:11, 436.36 examples/s]\u001b[A\n",
            "Generating dev.clean split:  13% 700/5589 [00:05<01:06, 73.99 examples/s] \u001b[A\n",
            "Generating dev.clean split:  18% 1000/5589 [00:05<00:28, 159.54 examples/s]\u001b[A\n",
            "Generating dev.clean split:  20% 1100/5589 [00:06<00:23, 192.63 examples/s]\u001b[A\n",
            "Generating dev.clean split:  25% 1400/5589 [00:06<00:13, 307.70 examples/s]\u001b[A\n",
            "Generating dev.clean split:  29% 1600/5589 [00:06<00:09, 401.68 examples/s]\u001b[A\n",
            "Generating dev.clean split:  32% 1800/5589 [00:06<00:08, 447.29 examples/s]\u001b[A\n",
            "Generating dev.clean split:  39% 2200/5589 [00:07<00:07, 443.72 examples/s]\u001b[A\n",
            "Generating dev.clean split:  45% 2500/5589 [00:07<00:05, 615.05 examples/s]\u001b[A\n",
            "Generating dev.clean split:  48% 2700/5589 [00:11<00:17, 168.13 examples/s]\u001b[A\n",
            "Generating dev.clean split:  52% 2900/5589 [00:12<00:12, 211.77 examples/s]\u001b[A\n",
            "Generating dev.clean split:  54% 3000/5589 [00:12<00:10, 239.98 examples/s]\u001b[A\n",
            "Generating dev.clean split:  55% 3100/5589 [00:14<00:19, 128.77 examples/s]\u001b[A\n",
            "Generating dev.clean split:  57% 3200/5589 [00:15<00:21, 112.79 examples/s]\u001b[A\n",
            "Generating dev.clean split:  64% 3600/5589 [00:16<00:08, 225.82 examples/s]\u001b[A\n",
            "Generating dev.clean split:  70% 3900/5589 [00:16<00:05, 301.40 examples/s]\u001b[A\n",
            "Generating dev.clean split:  72% 4000/5589 [00:22<00:17, 91.46 examples/s] \u001b[A\n",
            "Generating dev.clean split:  79% 4400/5589 [00:22<00:07, 165.30 examples/s]\u001b[A\n",
            "Generating dev.clean split:  86% 4800/5589 [00:22<00:03, 258.12 examples/s]\u001b[A\n",
            "Generating dev.clean split:  91% 5097/5589 [00:22<00:01, 348.35 examples/s]\u001b[A\n",
            "Generating dev.clean split:  95% 5294/5589 [00:22<00:00, 422.01 examples/s]\u001b[A\n",
            "Generating dev.clean split: 100% 5589/5589 [00:22<00:00, 243.24 examples/s]\n",
            "Setting num_proc from 8 to 3 for the test.clean split as it only contains 3 shards.\n",
            "03/16/2025 15:17:38 - WARNING - datasets.builder - Setting num_proc from 8 to 3 for the test.clean split as it only contains 3 shards.\n",
            "\n",
            "Generating test.clean split:   0% 0/4689 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating test.clean split:   2% 100/4689 [00:00<00:36, 124.12 examples/s]\u001b[A\n",
            "Generating test.clean split:   4% 200/4689 [00:01<00:23, 189.95 examples/s]\u001b[A\n",
            "Generating test.clean split:   9% 400/4689 [00:01<00:13, 312.15 examples/s]\u001b[A\n",
            "Generating test.clean split:  11% 500/4689 [00:01<00:12, 339.46 examples/s]\u001b[A\n",
            "Generating test.clean split:  13% 600/4689 [00:02<00:15, 266.94 examples/s]\u001b[A\n",
            "Generating test.clean split:  15% 700/4689 [00:02<00:15, 264.07 examples/s]\u001b[A\n",
            "Generating test.clean split:  17% 800/4689 [00:02<00:12, 321.96 examples/s]\u001b[A\n",
            "Generating test.clean split:  19% 900/4689 [00:03<00:10, 356.83 examples/s]\u001b[A\n",
            "Generating test.clean split:  21% 1000/4689 [00:03<00:09, 393.18 examples/s]\u001b[A\n",
            "Generating test.clean split:  23% 1100/4689 [00:03<00:12, 277.34 examples/s]\u001b[A\n",
            "Generating test.clean split:  26% 1200/4689 [00:04<00:10, 340.35 examples/s]\u001b[A\n",
            "Generating test.clean split:  28% 1300/4689 [00:07<00:39, 85.37 examples/s] \u001b[A\n",
            "Generating test.clean split:  30% 1400/4689 [00:07<00:28, 114.21 examples/s]\u001b[A\n",
            "Generating test.clean split:  34% 1600/4689 [00:07<00:15, 199.53 examples/s]\u001b[A\n",
            "Generating test.clean split:  36% 1700/4689 [00:07<00:12, 235.27 examples/s]\u001b[A\n",
            "Generating test.clean split:  41% 1900/4689 [00:07<00:08, 339.32 examples/s]\u001b[A\n",
            "Generating test.clean split:  45% 2100/4689 [00:08<00:06, 400.19 examples/s]\u001b[A\n",
            "Generating test.clean split:  51% 2400/4689 [00:08<00:04, 536.01 examples/s]\u001b[A\n",
            "Generating test.clean split:  55% 2600/4689 [00:08<00:03, 611.19 examples/s]\u001b[A\n",
            "Generating test.clean split:  58% 2700/4689 [00:08<00:03, 653.04 examples/s]\u001b[A\n",
            "Generating test.clean split:  64% 3000/4689 [00:09<00:03, 432.19 examples/s]\u001b[A\n",
            "Generating test.clean split:  66% 3100/4689 [00:10<00:03, 451.63 examples/s]\u001b[A\n",
            "Generating test.clean split:  68% 3200/4689 [00:13<00:11, 125.39 examples/s]\u001b[A\n",
            "Generating test.clean split:  77% 3600/4689 [00:13<00:04, 239.91 examples/s]\u001b[A\n",
            "Generating test.clean split:  81% 3800/4689 [00:16<00:06, 144.46 examples/s]\u001b[A\n",
            "Generating test.clean split:  83% 3900/4689 [00:17<00:05, 134.75 examples/s]\u001b[A\n",
            "Generating test.clean split:  92% 4300/4689 [00:17<00:01, 249.27 examples/s]\u001b[A\n",
            "Generating test.clean split:  95% 4463/4689 [00:17<00:00, 295.04 examples/s]\u001b[A\n",
            "Generating test.clean split: 100% 4689/4689 [00:18<00:00, 257.34 examples/s]\n",
            "\n",
            "Generating train.clean.100 split:   0% 0/32215 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   0% 100/32215 [00:02<15:02, 35.58 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   1% 200/32215 [00:02<06:43, 79.42 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   1% 300/32215 [00:03<03:56, 134.75 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   1% 400/32215 [00:03<02:58, 178.53 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   2% 500/32215 [00:04<03:07, 169.36 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   2% 600/32215 [00:05<05:11, 101.50 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   2% 700/32215 [00:19<05:10, 101.50 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   2% 800/32215 [00:26<29:25, 17.79 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:   3% 900/32215 [00:26<22:00, 23.72 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   3% 1100/32215 [00:28<14:41, 35.30 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   4% 1200/32215 [00:30<12:58, 39.82 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   4% 1400/32215 [00:33<11:18, 45.40 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   5% 1500/32215 [00:35<10:18, 49.63 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   5% 1600/32215 [00:35<08:09, 62.52 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   5% 1700/32215 [00:35<06:11, 82.15 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   6% 1800/32215 [00:41<12:24, 40.83 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   6% 1900/32215 [00:42<10:00, 50.52 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   6% 2000/32215 [00:42<08:00, 62.86 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   7% 2100/32215 [00:43<07:40, 65.34 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   7% 2200/32215 [00:44<06:37, 75.42 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   7% 2300/32215 [00:46<06:33, 75.93 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   7% 2400/32215 [00:50<11:25, 43.50 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   8% 2700/32215 [00:50<05:09, 95.36 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:   9% 2900/32215 [00:51<03:27, 141.02 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  10% 3200/32215 [00:56<05:35, 86.48 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  11% 3500/32215 [00:59<05:11, 92.15 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  11% 3600/32215 [00:59<04:35, 103.72 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  11% 3700/32215 [01:00<04:31, 105.14 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  12% 3900/32215 [01:00<03:04, 153.79 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  12% 4000/32215 [01:04<06:01, 78.06 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  13% 4100/32215 [01:05<05:46, 81.15 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  13% 4200/32215 [01:06<05:39, 82.42 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  13% 4300/32215 [01:07<05:07, 90.67 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  14% 4400/32215 [01:08<05:21, 86.48 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  14% 4500/32215 [01:10<06:19, 73.01 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  15% 4900/32215 [01:13<04:38, 98.18 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  16% 5000/32215 [01:14<04:32, 99.96 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  16% 5300/32215 [01:14<02:39, 168.85 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  17% 5400/32215 [01:18<05:14, 85.38 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  17% 5500/32215 [01:19<04:37, 96.14 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  17% 5600/32215 [01:19<03:53, 114.12 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  18% 5900/32215 [01:19<02:03, 213.25 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  19% 6100/32215 [01:25<05:04, 85.87 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  19% 6200/32215 [01:25<04:37, 93.70 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  20% 6300/32215 [01:28<05:47, 74.67 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  20% 6400/32215 [01:30<06:15, 68.79 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  20% 6500/32215 [01:30<04:53, 87.65 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  20% 6600/32215 [01:30<04:15, 100.16 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  21% 6700/32215 [01:31<03:37, 117.32 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  21% 6800/32215 [01:32<03:47, 111.74 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  21% 6900/32215 [01:35<06:18, 66.93 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  22% 7100/32215 [01:36<04:24, 95.05 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  22% 7200/32215 [01:38<05:25, 76.75 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  23% 7300/32215 [01:41<06:51, 60.54 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  23% 7400/32215 [01:41<05:25, 76.15 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  24% 7600/32215 [01:41<03:15, 125.75 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  24% 7800/32215 [01:42<02:40, 151.69 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  25% 7900/32215 [01:45<04:41, 86.28 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  25% 8100/32215 [01:46<03:50, 104.62 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  25% 8200/32215 [01:47<03:12, 124.56 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  26% 8300/32215 [01:49<04:52, 81.80 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  27% 8600/32215 [01:49<02:36, 150.89 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  27% 8700/32215 [01:55<06:30, 60.29 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  28% 9100/32215 [01:59<05:05, 75.59 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  29% 9400/32215 [01:59<03:16, 116.37 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  30% 9600/32215 [02:00<02:44, 137.29 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  30% 9700/32215 [02:03<03:57, 94.83 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  30% 9800/32215 [02:06<04:57, 75.28 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  31% 9900/32215 [02:06<04:24, 84.39 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  31% 10000/32215 [02:07<03:59, 92.62 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  31% 10100/32215 [02:07<03:12, 114.75 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  32% 10200/32215 [02:08<02:48, 130.37 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  32% 10300/32215 [02:08<02:31, 145.07 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  32% 10400/32215 [02:08<02:00, 180.34 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  33% 10500/32215 [02:10<03:04, 117.51 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  33% 10600/32215 [02:10<02:22, 151.57 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  34% 10800/32215 [02:10<01:37, 219.03 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  34% 10900/32215 [02:11<01:54, 186.88 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  34% 11000/32215 [02:12<01:37, 216.96 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  34% 11100/32215 [02:13<02:15, 156.37 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  35% 11200/32215 [02:15<03:57, 88.34 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  35% 11300/32215 [02:16<03:30, 99.33 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  36% 11500/32215 [02:18<03:28, 99.32 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  36% 11700/32215 [02:18<02:11, 156.41 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  37% 11800/32215 [02:18<01:55, 176.44 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  37% 11890/32215 [02:19<02:22, 142.75 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  38% 12090/32215 [02:20<01:33, 214.14 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  39% 12490/32215 [02:20<00:45, 431.21 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  39% 12670/32215 [02:20<00:38, 502.74 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  40% 12970/32215 [02:24<02:15, 142.20 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  41% 13070/32215 [02:25<02:19, 137.36 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  41% 13360/32215 [02:26<01:31, 206.33 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  42% 13460/32215 [02:26<01:23, 225.89 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  42% 13560/32215 [02:26<01:12, 258.56 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  43% 13850/32215 [02:26<00:53, 343.77 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  43% 13950/32215 [02:30<02:54, 104.65 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  44% 14039/32215 [02:31<02:46, 109.13 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  44% 14139/32215 [02:34<04:12, 71.60 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  45% 14639/32215 [02:35<01:50, 158.39 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  46% 14739/32215 [02:35<01:40, 173.04 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  46% 14839/32215 [02:40<04:02, 71.77 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  46% 14939/32215 [02:41<03:46, 76.40 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  47% 15039/32215 [02:42<03:22, 84.99 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  47% 15139/32215 [02:43<02:49, 100.62 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  47% 15239/32215 [02:44<03:00, 93.80 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  48% 15339/32215 [02:44<02:27, 114.19 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  48% 15439/32215 [02:44<01:58, 141.23 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  48% 15539/32215 [02:45<01:51, 149.78 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  49% 15639/32215 [02:47<03:13, 85.77 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  49% 15729/32215 [02:48<03:14, 84.74 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  49% 15929/32215 [02:49<01:47, 150.96 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  50% 16129/32215 [02:49<01:24, 189.68 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  51% 16329/32215 [02:49<00:58, 271.98 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  51% 16529/32215 [02:55<02:57, 88.40 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  52% 16629/32215 [02:55<02:35, 99.95 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  52% 16729/32215 [02:58<03:36, 71.60 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  52% 16818/32215 [03:01<04:38, 55.20 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  53% 17118/32215 [03:01<02:19, 107.97 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  54% 17318/32215 [03:05<03:07, 79.60 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  54% 17418/32215 [03:05<02:43, 90.64 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  54% 17518/32215 [03:10<04:27, 54.98 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  55% 17618/32215 [03:11<04:03, 59.97 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  55% 17718/32215 [03:11<03:11, 75.88 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  55% 17818/32215 [03:12<02:45, 86.78 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  56% 17918/32215 [03:13<02:23, 99.91 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  56% 18018/32215 [03:13<01:49, 130.10 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  56% 18118/32215 [03:15<02:53, 81.12 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  57% 18218/32215 [03:18<04:02, 57.62 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  57% 18318/32215 [03:19<03:34, 64.79 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  58% 18618/32215 [03:23<03:03, 73.98 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  58% 18718/32215 [03:24<02:56, 76.40 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  58% 18818/32215 [03:25<02:49, 79.04 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  59% 18918/32215 [03:26<02:21, 93.70 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  59% 19018/32215 [03:27<02:19, 94.61 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  59% 19118/32215 [03:27<01:49, 120.06 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  60% 19218/32215 [03:28<02:12, 98.21 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  60% 19318/32215 [03:29<02:16, 94.37 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  60% 19418/32215 [03:30<01:46, 120.16 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  61% 19518/32215 [03:32<02:35, 81.56 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  61% 19618/32215 [03:34<03:23, 62.00 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  61% 19718/32215 [03:36<03:07, 66.67 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  62% 20018/32215 [03:40<02:50, 71.58 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  63% 20318/32215 [03:40<01:46, 111.42 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  63% 20418/32215 [03:41<01:39, 118.12 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  64% 20518/32215 [03:41<01:26, 135.47 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  64% 20618/32215 [03:46<03:01, 63.85 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  64% 20718/32215 [03:47<02:38, 72.35 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  65% 20818/32215 [03:47<02:01, 94.16 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  65% 20918/32215 [03:47<01:33, 121.13 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  66% 21118/32215 [03:47<00:54, 204.08 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  66% 21218/32215 [03:49<01:19, 138.53 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  66% 21318/32215 [03:49<01:07, 160.85 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  66% 21418/32215 [03:50<01:20, 133.37 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  67% 21618/32215 [03:50<00:53, 198.91 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  67% 21718/32215 [03:55<02:26, 71.79 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  68% 22018/32215 [03:56<01:36, 105.18 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  69% 22118/32215 [03:56<01:21, 124.45 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  69% 22218/32215 [03:58<01:37, 102.20 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  69% 22318/32215 [03:59<01:28, 111.93 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  70% 22418/32215 [04:00<01:43, 95.05 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  70% 22518/32215 [04:00<01:21, 119.25 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  70% 22618/32215 [04:02<01:30, 105.97 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  71% 22718/32215 [04:06<02:57, 53.49 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  71% 22818/32215 [04:06<02:21, 66.53 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  71% 23018/32215 [04:07<01:26, 106.45 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  72% 23118/32215 [04:10<02:12, 68.66 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  72% 23218/32215 [04:10<01:49, 82.49 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  73% 23618/32215 [04:12<01:08, 125.46 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  74% 23718/32215 [04:15<01:27, 97.39 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  74% 23818/32215 [04:18<02:11, 64.02 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  74% 23918/32215 [04:19<01:50, 75.28 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  75% 24018/32215 [04:19<01:32, 89.09 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  75% 24318/32215 [04:19<00:45, 171.81 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  76% 24618/32215 [04:20<00:27, 273.31 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  77% 24707/32215 [04:23<01:02, 120.57 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  77% 24907/32215 [04:23<00:42, 174.00 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  78% 25107/32215 [04:24<00:41, 171.94 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  78% 25207/32215 [04:24<00:34, 202.05 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  79% 25307/32215 [04:27<01:05, 104.79 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  79% 25507/32215 [04:29<01:03, 106.20 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  79% 25607/32215 [04:29<01:00, 109.55 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  80% 25707/32215 [04:31<01:07, 96.18 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  80% 25907/32215 [04:32<00:50, 123.89 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  81% 26007/32215 [04:33<00:50, 124.07 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  81% 26107/32215 [04:36<01:28, 69.15 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  81% 26207/32215 [04:37<01:19, 75.89 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  82% 26507/32215 [04:41<01:11, 79.75 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  83% 26607/32215 [04:41<01:02, 89.70 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  84% 26997/32215 [04:41<00:28, 181.58 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  84% 27086/32215 [04:42<00:33, 153.79 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  84% 27176/32215 [04:43<00:34, 144.62 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  85% 27266/32215 [04:45<00:48, 102.81 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  85% 27356/32215 [04:46<00:43, 112.39 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  85% 27456/32215 [04:46<00:36, 131.51 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  86% 27656/32215 [04:47<00:25, 176.94 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  86% 27856/32215 [04:47<00:16, 257.85 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  87% 27956/32215 [04:47<00:16, 255.71 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  88% 28256/32215 [04:48<00:11, 348.24 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  88% 28456/32215 [04:48<00:08, 441.47 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  89% 28556/32215 [04:50<00:17, 207.93 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  89% 28656/32215 [04:51<00:26, 134.02 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  90% 28946/32215 [04:52<00:14, 232.99 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  90% 29135/32215 [04:55<00:28, 108.06 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  92% 29535/32215 [04:56<00:13, 201.29 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  92% 29735/32215 [05:02<00:28, 88.03 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  93% 29935/32215 [05:02<00:19, 117.62 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  94% 30135/32215 [05:06<00:24, 85.86 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  94% 30435/32215 [05:06<00:13, 133.71 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  95% 30635/32215 [05:06<00:09, 164.55 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  96% 30835/32215 [05:12<00:16, 83.01 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  96% 31035/32215 [05:12<00:10, 110.52 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  97% 31335/32215 [05:16<00:08, 98.39 examples/s] \u001b[A\n",
            "Generating train.clean.100 split:  98% 31435/32215 [05:16<00:06, 111.67 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  98% 31635/32215 [05:16<00:03, 155.01 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  98% 31725/32215 [05:18<00:04, 120.00 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  99% 31825/32215 [05:18<00:02, 137.01 examples/s]\u001b[A\n",
            "Generating train.clean.100 split:  99% 32025/32215 [05:18<00:00, 204.25 examples/s]\u001b[A\n",
            "Generating train.clean.100 split: 100% 32125/32215 [05:19<00:00, 185.01 examples/s]\u001b[A\n",
            "Generating train.clean.100 split: 100% 32215/32215 [05:22<00:00, 99.81 examples/s]\n",
            "\n",
            "Generating train.clean.360 split:   0% 0/112326 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   0% 100/112326 [00:02<51:41, 36.19 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   0% 300/112326 [00:02<14:15, 130.95 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   0% 400/112326 [00:03<10:06, 184.43 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   0% 500/112326 [00:07<35:49, 52.03 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   1% 600/112326 [00:12<53:08, 35.04 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 700/112326 [00:14<48:41, 38.21 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 800/112326 [00:14<34:11, 54.35 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 900/112326 [00:22<1:05:25, 28.38 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 1000/112326 [00:25<1:02:01, 29.92 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 1100/112326 [00:25<46:01, 40.28 examples/s]  \u001b[A\n",
            "Generating train.clean.360 split:   1% 1200/112326 [00:26<36:08, 51.25 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 1300/112326 [00:27<30:13, 61.22 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 1500/112326 [00:27<19:38, 94.04 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   1% 1600/112326 [00:29<19:49, 93.05 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 1700/112326 [00:30<20:06, 91.66 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 1800/112326 [00:30<15:20, 120.10 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2000/112326 [00:32<15:38, 117.61 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2200/112326 [00:33<15:22, 119.36 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2300/112326 [00:34<13:10, 139.26 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2400/112326 [00:34<10:51, 168.80 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2600/112326 [00:34<06:59, 261.83 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2700/112326 [00:34<06:46, 269.52 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   2% 2800/112326 [00:38<20:47, 87.79 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   3% 2900/112326 [00:40<25:48, 70.69 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   3% 3300/112326 [00:42<15:29, 117.33 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   3% 3400/112326 [00:43<15:21, 118.17 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   3% 3500/112326 [00:44<17:22, 104.34 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   3% 3700/112326 [00:45<12:42, 142.37 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   3% 3800/112326 [00:45<11:16, 160.42 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   3% 3900/112326 [00:45<11:14, 160.72 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   4% 4000/112326 [00:47<14:03, 128.36 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   4% 4100/112326 [00:50<26:28, 68.11 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   4% 4200/112326 [00:50<19:55, 90.45 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   4% 4600/112326 [00:50<08:06, 221.53 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   4% 4800/112326 [00:52<09:05, 197.11 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   4% 5000/112326 [00:54<12:23, 144.28 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5100/112326 [00:55<12:03, 148.16 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5200/112326 [00:55<10:05, 176.96 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5300/112326 [00:55<09:49, 181.43 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5400/112326 [00:57<14:52, 119.78 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5500/112326 [00:57<12:33, 141.81 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5600/112326 [00:58<14:56, 119.02 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5800/112326 [00:59<08:58, 197.64 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 5900/112326 [00:59<08:06, 218.66 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   5% 6100/112326 [00:59<05:11, 341.15 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   6% 6200/112326 [00:59<05:01, 351.50 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   6% 6300/112326 [00:59<04:22, 403.99 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   6% 6400/112326 [01:01<10:06, 174.78 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   6% 6500/112326 [01:03<18:38, 94.62 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   6% 6900/112326 [01:04<08:30, 206.47 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   6% 7000/112326 [01:14<39:47, 44.12 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   7% 7400/112326 [01:14<19:53, 87.91 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   7% 7600/112326 [01:14<14:50, 117.64 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   7% 7800/112326 [01:21<26:18, 66.23 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   7% 8000/112326 [01:26<31:59, 54.35 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   7% 8100/112326 [01:27<28:59, 59.92 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   7% 8300/112326 [01:33<36:04, 48.06 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   7% 8400/112326 [01:33<29:36, 58.52 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 8500/112326 [01:33<23:56, 72.29 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 8700/112326 [01:39<34:46, 49.66 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 9000/112326 [01:44<31:48, 54.13 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 9100/112326 [01:45<27:50, 61.79 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 9200/112326 [01:45<22:38, 75.92 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 9300/112326 [01:46<19:55, 86.21 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 9400/112326 [01:46<16:37, 103.22 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   8% 9500/112326 [01:50<28:26, 60.27 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   9% 9800/112326 [01:50<13:52, 123.18 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   9% 9900/112326 [01:56<31:37, 53.99 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:   9% 10300/112326 [02:02<28:43, 59.18 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:   9% 10400/112326 [02:06<35:12, 48.24 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 10700/112326 [02:06<21:29, 78.84 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 10800/112326 [02:11<32:00, 52.86 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 10900/112326 [02:12<28:14, 59.85 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 11100/112326 [02:12<18:54, 89.26 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 11200/112326 [02:18<35:09, 47.95 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 11300/112326 [02:19<28:39, 58.77 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 11500/112326 [02:24<34:51, 48.21 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  10% 11600/112326 [02:24<28:55, 58.04 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 11900/112326 [02:28<25:54, 64.62 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 12155/112326 [02:33<26:37, 62.69 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 12255/112326 [02:33<22:53, 72.87 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 12355/112326 [02:34<22:49, 73.01 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 12610/112326 [02:34<13:32, 122.70 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 12765/112326 [02:35<10:34, 156.81 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  11% 12865/112326 [02:35<09:37, 172.19 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13065/112326 [02:39<17:36, 93.94 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:  12% 13220/112326 [02:40<14:27, 114.19 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13320/112326 [02:41<15:52, 103.92 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13420/112326 [02:45<27:27, 60.05 examples/s] \u001b[A\n",
            "Generating train.clean.360 split:  12% 13520/112326 [02:46<27:14, 60.45 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13576/112326 [02:51<45:08, 36.45 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13776/112326 [02:52<26:21, 62.32 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13831/112326 [02:52<23:48, 68.95 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  12% 13931/112326 [02:56<34:31, 47.50 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14131/112326 [03:02<41:25, 39.50 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14231/112326 [03:02<33:58, 48.12 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14286/112326 [03:12<1:15:45, 21.57 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14386/112326 [03:16<1:08:49, 23.72 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14486/112326 [03:16<49:43, 32.79 examples/s]  \u001b[A\n",
            "Generating train.clean.360 split:  13% 14486/112326 [03:26<49:43, 32.79 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14586/112326 [03:30<1:42:41, 15.86 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14786/112326 [03:42<1:39:29, 16.34 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14886/112326 [03:42<1:15:51, 21.41 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 14986/112326 [03:43<1:02:18, 26.04 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  13% 15086/112326 [03:48<1:04:25, 25.15 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15186/112326 [03:48<48:49, 33.16 examples/s]  \u001b[A\n",
            "Generating train.clean.360 split:  14% 15286/112326 [03:52<54:01, 29.94 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15386/112326 [03:57<57:11, 28.25 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15486/112326 [04:05<1:19:42, 20.25 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15586/112326 [04:06<1:02:55, 25.63 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15686/112326 [04:07<49:56, 32.25 examples/s]  \u001b[A\n",
            "Generating train.clean.360 split:  14% 15786/112326 [04:09<41:34, 38.70 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15886/112326 [04:09<30:36, 52.51 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 15986/112326 [04:10<23:45, 67.57 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 16086/112326 [04:15<42:20, 37.88 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  14% 16186/112326 [04:21<59:04, 27.12 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  15% 16386/112326 [04:27<54:32, 29.32 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  15% 16586/112326 [04:38<1:05:18, 24.43 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  15% 16986/112326 [04:38<30:44, 51.68 examples/s]  \u001b[A\n",
            "Generating train.clean.360 split:  15% 17086/112326 [04:44<40:45, 38.95 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  15% 17286/112326 [04:50<43:26, 36.46 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 17641/112326 [04:50<25:17, 62.40 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 17741/112326 [04:56<34:39, 45.48 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 17841/112326 [05:00<39:11, 40.18 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 17941/112326 [05:00<31:55, 49.27 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 18141/112326 [05:06<36:37, 42.87 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 18241/112326 [05:06<30:33, 51.32 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 18341/112326 [05:06<24:29, 63.97 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  16% 18441/112326 [05:10<33:57, 46.08 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  17% 18541/112326 [05:16<49:13, 31.75 examples/s]\u001b[A\n",
            "Generating train.clean.360 split:  17% 18741/112326 [05:17<29:32, 52.80 examples/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1264, in wait\n",
            "Generating train.clean.360 split:  17% 18741/112326 [05:18<26:29, 58.87 examples/s]\n",
            "    return self._wait(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2053, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2011, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1172, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 759, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1277, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2047, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "Combining datasets...:   0% 0/4 [11:33<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 706, in iflatmap_unordered\n",
            "    yield queue.get(timeout=0.05)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<string>\", line 2, in get\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/managers.py\", line 822, in _callmethod\n",
            "    kind, result = conn.recv()\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 253, in recv\n",
            "    buf = self._recv_bytes()\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 433, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "          ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/connection.py\", line 398, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/parler-tts/./training/run_parler_tts_training.py\", line 1249, in <module>\n",
            "    main()\n",
            "  File \"/content/parler-tts/./training/run_parler_tts_training.py\", line 247, in main\n",
            "    raw_datasets[\"train\"] = load_multiple_datasets(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/training/data.py\", line 208, in load_multiple_datasets\n",
            "    dataset = load_dataset(\n",
            "              ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 2083, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 925, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1001, in _download_and_prepare\n",
            "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1771, in _prepare_split\n",
            "    for job_id, done, content in iflatmap_unordered(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 720, in iflatmap_unordered\n",
            "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 720, in <listcomp>\n",
            "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 770, in get\n",
            "    raise TimeoutError\n",
            "multiprocess.context.TimeoutError\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mMini\u001b[0m at: \u001b[34mhttps://wandb.ai/sivakgp-iit-kharagpur/parler-tts-50k-hours/runs/j2wh1yk9\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250316_151703-j2wh1yk9/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# run the model with json file directly\n",
        "!accelerate launch ./training/run_parler_tts_training.py ./helpers/training_configs/starting_point_v1.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSEqjUpRztMY"
      },
      "source": [
        "# Hugging face log in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22eL-MsmzsTF",
        "outputId": "83be9c84-9649-40f1-a354-950f97c9176c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "The token `Indic-parler-tts` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Indic-parler-tts`\n"
          ]
        }
      ],
      "source": [
        "# log in to hugging face\n",
        "!huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa21LqS9468K"
      },
      "source": [
        "# Use trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfepT-CEfYNY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlVqg784fYSl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6N1TbcnfYVM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc11hGFqfYXz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02b8c6a2d8ec4fdb8b73206492687620": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_913ca49827d54b26a00e3d9913e7e739",
            "placeholder": "​",
            "style": "IPY_MODEL_975d7515286246fe9195f91a950e30d4",
            "value": " 5.17k/5.17k [00:00&lt;00:00, 580kB/s]"
          }
        },
        "08185c28a30445b488b0aa9872d8117a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1079f7edd7354a678887d97f2ce4949f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a28f573d2c1e46449f6f92d7b3aabe27",
            "placeholder": "​",
            "style": "IPY_MODEL_17a29ce59379495a9500e9e35bfa34ab",
            "value": " 7.42k/7.42k [00:00&lt;00:00, 178kB/s]"
          }
        },
        "16a1b36032084854943e90bd53283438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17a29ce59379495a9500e9e35bfa34ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bc40bc56b5349ff81b4eb430057b4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f93a2c0067a4eaaa8207ac9d6d76048": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_250c5a1aef914c1ab955ab7220be6693",
            "placeholder": "​",
            "style": "IPY_MODEL_ec053e534edb481d97c3b564f1598742",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 6.77MB/s]"
          }
        },
        "242b408e32a343f1acb83e69f68ce562": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a4de54047d408b86b5cbdacbfe1608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beb1e9a8397f48079afc51ba6981f6f5",
              "IPY_MODEL_2d04a2ce782a4a828740db8cd2b5e37d",
              "IPY_MODEL_cc29f24d72ec4d1f86eba5c7c4e149a1"
            ],
            "layout": "IPY_MODEL_f049191491b944e5acdba4f8ce4479d7"
          }
        },
        "250c5a1aef914c1ab955ab7220be6693": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "259acf4ea6f0467782e85db1d7606950": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d04a2ce782a4a828740db8cd2b5e37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9faa2062f559458699cf961012b8dd52",
            "max": 1875714666,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44d0174046624262b87bf50eadcf4847",
            "value": 1875714666
          }
        },
        "2df5f63ac18c484ba6dfda2965879300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5c3ba2df64414e92bb0e84e362382a",
            "placeholder": "​",
            "style": "IPY_MODEL_cbb16d2f9fdf41698624e99d0535d7d6",
            "value": "README.md: 100%"
          }
        },
        "2e12d0380c0d4b41adcd0b3b17640815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9eac6e96dae4cffb8a714d0034238c5",
            "placeholder": "​",
            "style": "IPY_MODEL_bf42a44ae79e41fa9c5d5cf3e2c6054b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "3037060c415e47efb2ee21745acf81e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3191fb99ae9d4e1c84bda97e24b2a9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae6377bf6f9e47ddacb357cbbfa0c474",
              "IPY_MODEL_8d4e6e609e434f4a9eaa5b9fe9754037",
              "IPY_MODEL_1079f7edd7354a678887d97f2ce4949f"
            ],
            "layout": "IPY_MODEL_d5773a2bfe0c439b913f61c1151f1366"
          }
        },
        "33a8cd4dd8db4359ab4a7abe6c9f3b35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ca53d0bbc294e55a3a565f8fe4fd821": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdb25fc87c14b578846c387a00add97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f828f78560447b5883aee85e5cb9add": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f749b7033a0749828067eb03354fcaf4",
            "max": 5174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aa54fa766ea4ed9af6e5e6ec62fdb6c",
            "value": 5174
          }
        },
        "44d0174046624262b87bf50eadcf4847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47a45d29afd1417da411a3e843389ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f6bcdf086f04025b7d7858b75dfd64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a1fa63223ea445e87d3563a9224c3f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4fbca3dc984ec1b88868307779e29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f010a88d0e24ece82853643c5e90488",
            "placeholder": "​",
            "style": "IPY_MODEL_85ade18e00a74052879e1f2162ee5a70",
            "value": "tokenizer.model: 100%"
          }
        },
        "5a5c3ba2df64414e92bb0e84e362382a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d59aca23c9944cd929ec4785a34bb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_242b408e32a343f1acb83e69f68ce562",
            "placeholder": "​",
            "style": "IPY_MODEL_1bc40bc56b5349ff81b4eb430057b4a2",
            "value": ""
          }
        },
        "5f010a88d0e24ece82853643c5e90488": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64654447d6834e179fba305bf85b494c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_91b4817f129e4f98a1e95c9af9a233e2",
            "style": "IPY_MODEL_7968ccc41bb743469499927a15aa6c5d",
            "value": true
          }
        },
        "7968ccc41bb743469499927a15aa6c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6e87521e1f441b9618bd2bb1f121b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e2427176de348bdaad83c5916efa438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_956922f09c194fc18d880e21298ab9a9"
          }
        },
        "852df91b17a4446f90a6fd47109dd7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e23e7d20c04600ae13dae5d34887ff",
            "max": 1795391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3037060c415e47efb2ee21745acf81e7",
            "value": 1795391
          }
        },
        "85ade18e00a74052879e1f2162ee5a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aa54fa766ea4ed9af6e5e6ec62fdb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d4e6e609e434f4a9eaa5b9fe9754037": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecfe7b861893463ca803f212521ea86f",
            "max": 7424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e54b7fef56eb482c8aa6fe2fd47d42f2",
            "value": 7424
          }
        },
        "913ca49827d54b26a00e3d9913e7e739": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91b4817f129e4f98a1e95c9af9a233e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "956922f09c194fc18d880e21298ab9a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "975d7515286246fe9195f91a950e30d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ce58ce4b96646b49b8ae0226df981f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a4fbca3dc984ec1b88868307779e29e",
              "IPY_MODEL_852df91b17a4446f90a6fd47109dd7ec",
              "IPY_MODEL_1f93a2c0067a4eaaa8207ac9d6d76048"
            ],
            "layout": "IPY_MODEL_d756561f2631495e96c1fada753cec93"
          }
        },
        "9faa2062f559458699cf961012b8dd52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a09fa8cfca9c4d558447a1bfc2e1f732": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08185c28a30445b488b0aa9872d8117a",
            "placeholder": "​",
            "style": "IPY_MODEL_259acf4ea6f0467782e85db1d7606950",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a28f573d2c1e46449f6f92d7b3aabe27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9b840505764396890248e2214dcf43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab55cbadd183444093ed27c56611f6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ae6377bf6f9e47ddacb357cbbfa0c474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb5fb84e7b1d4137a5a7e36cd179ed7d",
            "placeholder": "​",
            "style": "IPY_MODEL_16a1b36032084854943e90bd53283438",
            "value": "config.json: 100%"
          }
        },
        "beb1e9a8397f48079afc51ba6981f6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdb25fc87c14b578846c387a00add97",
            "placeholder": "​",
            "style": "IPY_MODEL_47a45d29afd1417da411a3e843389ede",
            "value": "model.safetensors: 100%"
          }
        },
        "bf42a44ae79e41fa9c5d5cf3e2c6054b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c39f98d356b54f80bfa27fdaed44a7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2df5f63ac18c484ba6dfda2965879300",
              "IPY_MODEL_3f828f78560447b5883aee85e5cb9add",
              "IPY_MODEL_02b8c6a2d8ec4fdb8b73206492687620"
            ],
            "layout": "IPY_MODEL_3ca53d0bbc294e55a3a565f8fe4fd821"
          }
        },
        "cbb16d2f9fdf41698624e99d0535d7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc29f24d72ec4d1f86eba5c7c4e149a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa9b840505764396890248e2214dcf43",
            "placeholder": "​",
            "style": "IPY_MODEL_7c6e87521e1f441b9618bd2bb1f121b1",
            "value": " 1.88G/1.88G [01:00&lt;00:00, 35.3MB/s]"
          }
        },
        "d5773a2bfe0c439b913f61c1151f1366": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d756561f2631495e96c1fada753cec93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0deb28fbfa7480699795674615477f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1fa63223ea445e87d3563a9224c3f7",
            "placeholder": "​",
            "style": "IPY_MODEL_4f6bcdf086f04025b7d7858b75dfd64c",
            "value": "Connecting..."
          }
        },
        "e4f14546fa664961a7126613755dfa8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_33a8cd4dd8db4359ab4a7abe6c9f3b35",
            "style": "IPY_MODEL_ab55cbadd183444093ed27c56611f6f3",
            "tooltip": ""
          }
        },
        "e54b7fef56eb482c8aa6fe2fd47d42f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6e23e7d20c04600ae13dae5d34887ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9eac6e96dae4cffb8a714d0034238c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec053e534edb481d97c3b564f1598742": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecfe7b861893463ca803f212521ea86f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f049191491b944e5acdba4f8ce4479d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f749b7033a0749828067eb03354fcaf4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5fb84e7b1d4137a5a7e36cd179ed7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
